{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A: Homework 4\n",
    "# Regularization, High Dimensionality, PCA\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine\n",
    "\n",
    "---\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "- Do not include your name(s) in the notebook even if you are submitting as a group. \n",
    "- If you submit individually and you have worked with someone, please include the name of your [one] partner below. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your partner's name (if you submit separately):\n",
    "\n",
    "Enrollment Status (109A, 121A, 209A, or E109A): 109A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: All answers are in green font"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuing Bike Sharing Usage Data\n",
    "\n",
    "In this homework, we will focus on multiple linear regression, regularization, dealing with high dimensionality, and PCA. We will continue to build regression models for the Capital Bikeshare program in Washington D.C.  See Homework 3 for more information about the data.\n",
    "\n",
    "*Note: please make sure you use all the processed data from HW 3 Part (a)...you make want to save the data set on your computer and reread the csv/json file here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Biketrain = pd.read_csv('Bikeshare_train.csv')\n",
    "Biketest = pd.read_csv('Bikeshare_test.csv')\n",
    "y_train = Biketrain['count'].values\n",
    "y_test = Biketest['count'].values\n",
    "X_train = pd.read_pickle('Xtrain')\n",
    "X_test = pd.read_pickle('Xtest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (f): Regularization/Penalization Methods\n",
    "\n",
    "As an alternative to selecting a subset of predictors and fitting a regression model on the subset, one can fit a linear regression model on all predictors, but shrink or regularize the coefficient estimates to make sure that the model does not \"overfit\" the training set. \n",
    "\n",
    "Use the following regularization techniques to fit linear models to the training set:\n",
    "- Ridge regression\n",
    "- Lasso regression\n",
    "    \n",
    "You may choose the shrikage parameter $\\lambda$ from the set $\\{10^{-5}, 10^{-4},...,10^{4},10^{5}\\}$ using cross-validation. In each case, \n",
    "\n",
    "- How do the estimated coefficients compare to or differ from the coefficients estimated by a plain linear regression (without shrikage penalty) in Part (b) fropm HW 3? Is there a difference between coefficients estimated by the two shrinkage methods? If so, give an explantion for the difference.\n",
    "- List the predictors that are assigned a coefficient value close to 0 (say < 1e-10) by the two methods. How closely do these predictors match the redundant predictors (if any) identified in Part (c) from HW 3?\n",
    "- Is there a difference in the way Ridge and Lasso regression assign coefficients to the predictors `temp` and `atemp`? If so, explain the reason for the difference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next analyze the performance of the two shrinkage methods for different training sample sizes:\n",
    "- Generate random samples of sizes 100, 150, ..., 400 from the training set. You may use the following code to draw a random sample of a specified size from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#--------  sample\n",
    "# A function to select a random sample of size k from the training set\n",
    "# Input: \n",
    "#      x (n x d array of predictors in training data)\n",
    "#      y (n x 1 array of response variable vals in training data)\n",
    "#      k (size of sample) \n",
    "# Return: \n",
    "#      chosen sample of predictors and responses\n",
    "\n",
    "def sample(x, y, k):\n",
    "    n = x.shape[0] # No. of training points\n",
    "    \n",
    "    # Choose random indices of size 'k'\n",
    "    subset_ind = np.random.choice(np.arange(n), k)\n",
    "    \n",
    "    # Get predictors and reponses with the indices\n",
    "    x_subset = x[subset_ind, :]\n",
    "    y_subset = y[subset_ind]\n",
    "    \n",
    "    return (x_subset, y_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fit linear, Ridge and Lasso regression models to each of the generated sample. In each case, compute the $R^2$ score for the model on the training sample on which it was fitted, and on the test set.\n",
    "- Repeat the above experiment for 10 random trials/splits, and compute the average train and test $R^2$ across the trials for each training sample size. Also, compute the standard deviation (SD) in each case.\n",
    "- Make a plot of the mean training $R^2$ scores for the linear, Ridge and Lasso regression methods as a function of the training sample size. Also, show a confidence interval for the mean scores extending from **mean - SD** to **mean + SD**. Make a similar plot for the test $R^2$ scores.\n",
    "\n",
    "How do the training and test $R^2$ scores compare for the three methods? Give an explanation for your observations. How do the confidence intervals for the estimated $R^2$ change with training sample size? Based on the plots, which of the three methods would you recommend when one needs to fit a regression model using a small training sample?\n",
    "\n",
    "*Hint:* You may use `sklearn`'s `RidgeCV` and `LassoCV` classes to implement Ridge and Lasso regression. These classes automatically perform cross-validation to tune the parameter $\\lambda$ from a given range of values. You may use the `plt.errorbar` function to plot confidence bars for the average $R^2$ scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (f) Code and Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LassoCV(alphas=[1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000],\n",
       "    copy_X=True, cv=None, eps=0.001, fit_intercept=True, max_iter=1000,\n",
       "    n_alphas=100, n_jobs=1, normalize=False, positive=False,\n",
       "    precompute='auto', random_state=None, selection='cyclic', tol=0.0001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ridge and Lasso\n",
    "alphas = [10**(i) for i in range(-5,6)]\n",
    "linreg = LinearRegression(fit_intercept = True)\n",
    "linreg.fit(X_train.values,y_train)\n",
    "ridge = RidgeCV(alphas = alphas,fit_intercept = True)\n",
    "ridge.fit(X_train.values,y_train)\n",
    "lassr = LassoCV(alphas = alphas,fit_intercept = True)\n",
    "lassr.fit(X_train.values,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  898.82903949,  1032.88157484,  1226.18654265,    88.94009267,\n",
       "         239.18089841,   333.34908642,   -65.81249999,  -792.2628985 ,\n",
       "       -1279.98700626,  -776.4754899 ,   405.1445661 ,   486.25090367,\n",
       "         112.68164522,  -118.83581872,  -284.35627461,  -123.75147639,\n",
       "        -195.28593262,   170.51134687,    61.25602987,   111.06689939,\n",
       "         465.14500996,   308.15314174,   -16.56658402, -1581.97828361,\n",
       "         925.73384986,   312.43407189,  -548.49294906,  -255.12258899])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 393.17255806,  172.1792805 ,  761.78543763, -115.1218662 ,\n",
       "         88.89050681,  369.53564813,  133.37245386, -312.37487384,\n",
       "       -529.54451686,  -89.29404068,  676.88686869,  503.42304275,\n",
       "        159.09586506, -100.6582173 , -158.55971554, -130.58113365,\n",
       "       -125.49534264,  124.31796857,   64.47904074,  126.17310553,\n",
       "        303.9886922 ,  217.45335409,   20.28051056, -676.14388831,\n",
       "        682.80825822,  553.88994683, -567.72191933, -266.41311936])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  543.86140467,   113.75783363,   898.93231611,    -0.        ,\n",
       "          13.92014333,   312.54570464,     0.        ,  -354.60286434,\n",
       "        -496.15194078,    -0.        ,   834.58741964,   483.88249353,\n",
       "          68.02952205,    -0.        ,    -0.        ,  -178.96215235,\n",
       "        -129.13868319,    17.59275663,     0.        ,    11.32761788,\n",
       "         309.95116311,   277.51880871,    -0.        , -1058.29476303,\n",
       "         854.7339104 ,   399.36424117,  -555.84074594,  -254.3687472 ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lassr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q\n",
    "- How do the estimated coefficients compare to or differ from the coefficients estimated by a plain linear regression (without shrikage penalty) in Part (b) fropm HW 3? Is there a difference between coefficients estimated by the two shrinkage methods? If so, give an explantion for the difference.\n",
    "\n",
    "A\n",
    "\n",
    "- What we can see above is that the ridge coefficients are generally smaller than the normal linear regression from part 3b). We can also see that Lasso makes some of the coefficients smaller. There is a difference between lasso and ridge in that lasso adds a linear term to the cost function while ridge adds a squared term. The difference between these methods means that Lasso is more likely to suppress coefficients down to zero in relation to ridge as we saw in class with an ellipse lying tangent to a square/circle.\n",
    "\n",
    "Q\n",
    "- List the predictors that are assigned a coefficient value close to 0 (say < 1e-10) by the two methods. How closely do these predictors match the redundant predictors (if any) identified in Part (c) from HW 3?\n",
    "\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso coefficients that are ~ 0: ['month_2' 'month_5' 'month_8' 'month_12' 'holiday_1' 'day_of_week_4'\n",
      " 'weather_2']\n"
     ]
    }
   ],
   "source": [
    "print('Lasso coefficients that are ~ 0: %s' %X_train.columns.values[np.where(abs(lassr.coef_)<10**(-10))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- In part c the statistically significant predictors were 'season4','weather3','humidity','windspeed', none of which is listed above in the predictors dropped by lasso which means lasso and t-test dropped the same predictors with t-test dropping more than lasso. Ridge does not drop any columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q\n",
    "- Is there a difference in the way Ridge and Lasso regression assign coefficients to the predictors `temp` and `atemp`? If so, explain the reason for the difference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A\n",
    "\n",
    "- The ridge regressor assigns similar coefficients to temp and atemp. While Lasso assigns a larger coefficient to temp than to atemp. The key difference here is that ridge tries to solve the issue of multi collinearity by equally weighting similar predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- # Training sample size variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Shaan Desai\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "alphas = [10**(i) for i in range(-5,6)]\n",
    "linreg = LinearRegression(fit_intercept = True)\n",
    "ridge = RidgeCV(alphas = alphas,fit_intercept = True)\n",
    "lassr = LassoCV(alphas = alphas,fit_intercept = True)\n",
    "\n",
    "samples = [100+50*i for i in range(7)]\n",
    "r2train  = np.zeros((3,len(samples),10))\n",
    "r2test  = np.zeros((3,len(samples),10))\n",
    "\n",
    "for i in range(len(samples)):\n",
    "    for j in range(10):\n",
    "        x_sub, y_sub = sample(X_train.values,y_train,samples[i])\n",
    "        linreg.fit(x_sub,y_sub)\n",
    "        ridge.fit(x_sub,y_sub)\n",
    "        lassr.fit(x_sub,y_sub)\n",
    "        r2train[0,i,j] = r2_score(y_sub,linreg.predict(x_sub))\n",
    "        r2train[1,i,j] = r2_score(y_sub,ridge.predict(x_sub))\n",
    "        r2train[2,i,j] = r2_score(y_sub,lassr.predict(x_sub))\n",
    "        r2test[0,i,j] = r2_score(y_test,linreg.predict(X_test.values))\n",
    "        r2test[1,i,j] = r2_score(y_test,ridge.predict(X_test.values))\n",
    "        r2test[2,i,j] = r2_score(y_test,lassr.predict(X_test.values))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1e96c085e80>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl4VdXZ9/HvjwAGwYIyyKhAq1SEEDDiAI4ooCCiRaHO\nWqcqytPXxqEOjdZWWhyK1hYHnFHw4VFEQ8HiPMtoAAUHihJEBCoIFISE+/1j78STcJLshJwk5+T+\nXNe5cvba01rZybnPXmuvtWRmOOeccxVpUNsZcM45lxw8YDjnnIvEA4ZzzrlIPGA455yLxAOGc865\nSDxgOOeci8QDhtuFpDRJmyXtV53bOgcg6WJJryfguMdKWlLdx3U/8oCRAsIP7KLXTklbY5bPruzx\nzKzQzJqZ2VfVuW1lSbpdkkm6slT6NWH6TdV9zgh5yo/5/X4j6RFJTeNst4ekVyQVSDo1zvrrJS2R\ntEnSckn/rwby3lPSvyR9F77mShqU6PNWp/LKYGavm9nBtZ3HVOYBIwWEH9jNzKwZ8BVwSkzapNLb\nS2pY87mssk+B80qlnRem15aTwt91H6AvcG3sSkkNgKeAAuAXwCOS+sc5zjlAC2AI8BtJIxKVYUkC\nXgJmAG2AtsBvgM2JOmd1S4UyJDsPGPVA+E19iqRnJG0CzpF0hKT3JW2QtFrSvZIahds3DL/Bdw6X\nnwrX/zP8RvyepC6V3TZcf5KkTyVtlHSfpHckXVBO9t8D9pHULdw/k+DvdkGpMg6T9FFYnrcl9YhZ\nd1P4LX5T+K1+WMy6iyW9IemecN/lkgZG+b2a2dfAy0BmqVX3A42BYWb2AnA68HRsnsxsrJktCO/Q\nPgFeBPrFO0/4jfryUmmLwzI3CH/f34a/0zxJ3eMcZl9gP+AhM9thZj+Y2Vtm9k54vJaSZkhaG35z\nf1FSh5jzvS3ptvBvZoukaeE+z0j6XtIHCqslY/4mrpL0b0nrJI0NA2m88nWXNFvSfyQtlfSLMn7l\nFZXhBEkrwvdnq+Sd9w+SZofr0iXdLWmlpDWS/i4pvYxzuhgeMOqP04CngebAFIJvv2OAVgQfVIOB\ny8rZ/yzgZmAfgruYP1R2W0ltgGeB7PC8/yb4hl6RJ/nxLuM84InYlZIOBR4CLgZaAo8AL0hqHG7y\naVjG5sAfCT689405xJHAonDfe4CJEfKEpE4Ev7fPY9J+Gi6PMLMfAMzsDYI7jcPLOE4DoD9QVv37\nM8AvY7bvBbQDZgInhcc9ANgbGAX8J84xvgWWA5MknRpei1gNCH6H+wH7AzuA8aW2GUlwbTsCPwfe\nBR4kuM5fEFzzWKcS3IUdAoxg1ztFJDUD/kVwTdsAZwMPFn1BqGQZipnZpJi77o7ACoLfI8A4oAuQ\nQfB76wzcWNaxXAwz81cKvQj+MU4olXY78GoF+/0W+N/wfUPAgM7h8lPAhJhthwGLq7DtRcBbMesE\nrAYuKCNPtwOPEfxzrwAaAauA9sBk4KZwu4eA35fa9wugXxnHXQwMCd9fDCyNWfeTsDytytg3n6AK\nZFO43ctA8928Zn8E5gONy1jfHPgv0DFc/jPwYPh+ILAUOAxoUMF5OgF/J/jQLQReA35axrZZwNqY\n5beB62KWxwMvxiyfBswt9TdxQsz6q4FZMb/z18P3ZwOvlTr3RODGypYBOAFYUWr7BgSB9b6Y5W3A\n/jHbHAV8lqj/yVR6+R1G/bEydkHSzyXlKmi4/R64jeBbf1m+iXn/X6BZFbZtH5sPC/5b8yvKuJn9\nm+BO5U8EwefrUpvsD1wXViltkLSB4Bt4BwBJF8RUV20g+HYcW9bS+aWC8g01s72AAUB3gm/YVSJp\nDMFdwVAz2x5vGzPbSPChN1KSwu0nheteBiYA/wDWSJogaa8yjrPSzK4ws64EQXgHQUBGUjNJD0v6\nKvx7eJVd/x7WxLzfGme59O8s9m/uS4LrX9r+QL9S124kwfWrVBnK8GeC6sHfhMttgT2A2L+Hlwju\nblwFPGDUH6WHJX6A4Jv2z8zsJ8AtBN/4E2k1QfUAUNyI2aHszUt4AriGUtVRoZXArWbWIua1p5k9\nK6krwYfpr4GWZtaC4Bv5bpfVzF4l+OAeV5X9JV1KUKYBcYJgaUXVUv0J/m/fjMnHX82sD9CDIIBV\n+MSVBU+1/T3cB4Jqwi5A3/Dv4fjKlSauTjHv9wPilXEl8Eqpa9fMzEZXdPA4ZShB0jkEVYFnmFlB\nmLwG2A50izlfczNrXoly1VseMOqvvYCNwBZJB1F++0V1eQnoI+kUBU9qjQFaR9z3aYLql/+Ls+4h\n4EpJhyrQLDxHU4JvvQasJYhRlxDcYVSXe4CTYxu0o5B0PnArcKKZrYiwy4sE9e23AJPDuzMk9Q1f\nDYEtBB+GO+Ocr5Wk30vqGv6OWgMXAu+Hm+xFcHf1naSW4Xl217WSWoSN4VcTtJ2VNh04WNJZkhqF\nr77x2jAilCF22yyCa3Oqma0vSjezQuBh4K+SWofH6aiIDzrUdx4w6q9rgPMJ6uIfIP4/c7UyszUE\n1Q13A+uBnxI87fRDhH3/a2azzWxbnHXvE9xB/AP4jqCR+5xwXR5wH/AhwR1ON+CD6ihPePxvCO4y\nSjf4VuR2gkb2eTFP8vytnPNsA6YR1NM/HbOqBUGd/waCdp7VBL/f0n4g+H2/RtAGsyj8eVG4/m6C\ntpL1BI3Z/6xkeeJ5EVhIcI2fJ07VUVjdNojgeq0mqB68g6DaqLJliDWc4CGA92J+vy+G664hqCL7\nkOBL08sEwdhVQOEXFedqnKQ0gmqKEWb2Vm3nx1WP8G5nB9Al4t2TSxJ+h+FqlKTBYTXFHgTfyncQ\nfNNzztVxHjBcTetP8EjkWoKqiNMs7K/gnKvbvErKOedcJH6H4ZxzLpJkGoSuQq1atbLOnTvXdjac\ncy5pzJs3b52ZRXq8PaEBQ9JggiEE0oCHzWxsqfXZBEMDFOXlIKC1mf0nHERsE0H3/wIzy6rofJ07\nd2bu3LnVWALnnEttkr6Mum3CAkb4yOT9wIkEwz/MkTTdzD4u2sbMxhH2kpV0CvAbM4sdOO04M1uX\nqDw655yLLpFtGH2Bz81seThGzmSC0SvL8kt+HE3SOedcHZPIgNGBkoOP5VPGuEGS9iQYJjp22AcD\nZkuaF465E5ekSxXMujV37dq11ZBt55xz8dSVRu9TgHdKVUf1N7NV4Zj3/5K01MzeLL2jmT1IMCY/\nWVlZ/oywcwm0Y8cO8vPz2bZtlxFaXB2Xnp5Ox44dadSoUZWPkciAsYqSo1V2DNPiGUWp6igzWxX+\n/FbS8wRVXLsEDOdczcnPz2evvfaic+fOBIMNu2RgZqxfv578/Hy6dOlS8Q5lSGSV1BzgAEldwpnP\nRhGMTFmCpObAMcALMWlNi8b0D0ccHUgwFLdzrhZt27aNli1berBIMpJo2bLlbt8ZJuwOw8wKJI0G\nZhE8VvuImS1RODexmU0INz0NeNnMtsTsvi/wfPhH2RB42sxmJiqvzrnoPFgkp+q4bgltwzCzGcCM\nUmkTSi0/Rqlhj81sOdArkXlzztWMkQ+8B8CUy46o5Zy43eVDgxD8QRf9UTvn6rZmzYKZYL/++mtG\njBiR8PMtXLiQGTNmVLxhKVXJ37HHHku3bt3o1asXhx56KAsXLtxlm8suu4ymTZvy6quvlki/++67\n6d69OxkZGQwYMIAvv4zcHy8yDxjOuYSZtmAVC77awAf//g/9xr7KtAVlPfdSee3bt2fq1KmV2qew\nsLDS5ykvYBQUFMRNh6rlD2DSpEl89NFHXHHFFWRnZ5dYd/vtt7NhwwY++OADrrzySvLy8orX9e7d\nm7lz55KXl8eIESO49tprK33uinjAcM4lxLQFq7jhuUVsLwxmjF21YSs3PLeo2oLGihUr6NEjmBn3\nscce4/TTT2fw4MEccMABJT4smzVrxjXXXEOvXr147733mDdvHscccwyHHHIIgwYNYvXq1QDMmTOH\njIwMMjMzyc7OpkePHmzfvp1bbrmFKVOmkJmZyZQpU8jJyeHcc8+lX79+nHvuuaxYsYKjjjqKPn36\n0KdPH959991K5a8sRxxxBKtW/fi7evzxx1myZAlPP/00PXr0YPr06VxyySWsXBl0dzvuuOPYc889\nATj88MPJz8+vht9ySR4wnHMJMW7WMrbuKPmNfuuOQsbNWpaQ8y1cuJApU6awaNEipkyZUvxBumXL\nFg477DA++ugjDjvsMK666iqmTp3KvHnzuOiii7jxxhsBuPDCC3nggQdYuHAhaWlpADRu3JjbbruN\nkSNHsnDhQkaOHAnAxx9/zOzZs3nmmWdo06YN//rXv5g/fz5Tpkzh6quvrlT+yjJz5kyGDx9evHz+\n+efzzDPPFOftgAMO4IMPPqBTp0677Dtx4kROOumkSv4GK1ZXOu4551LM1xu2Vip9dw0YMIDmzZsD\n0L17d7788ks6depEWloav/jFLwBYtmwZixcv5sQTTwSCKqp27dqxYcMGNm3axBFHBA3zZ511Fi+9\n9FKZ5xo2bBhNmjQBgs6Mo0ePLg40n376aaXyV9rZZ5/N9u3b2bx5c9w2jIo89dRTzJ07lzfeeKPS\n+1bEA4ZzLiHat2jCqjjBoX2LJgk53x577FH8Pi0trbh9IT09vfhbuZlx8MEH8957JR9y2bBhQ6XO\n1bRp0+L399xzD/vuuy8fffQRO3fuJD09vVL5K23SpEkccsghZGdnc9VVV/Hcc89Fztfs2bP54x//\nyBtvvFHifNXFq6SccwmRPagbTRqllUhr0iiN7EHdailH0K1bN9auXVscMHbs2MGSJUto0aIFe+21\nFx988AEAkydPLt5nr732YtOmTWUec+PGjbRr144GDRrw5JNPVqlhvTRJ/OEPf+D9999n6dKlkfZZ\nsGABl112GdOnT6dNmza7nYd4PGA45xJieO8O3HF6TxqnBR8zHVo04Y7TezK8d9wxSGtE48aNmTp1\nKtdddx29evUiMzOzuJF64sSJXHLJJWRmZrJly5bi6qPjjjuOjz/+uLjRu7QrrriCxx9/nF69erF0\n6dISdx+7o0mTJlxzzTWMGzcu0vbZ2dls3ryZM844g8zMTIYNG1Yt+YiVUnN6Z2VlWVUmUPKORc5F\n88knn3DQQQdVap9k+f/avHlzcR+PsWPHsnr1asaPH1/Luape8a6fpHlRJqgDb8NwziVYXQ8URXJz\nc7njjjsoKChg//3357HHHqvtLNU5HjCccw4YOXJk8WOzLj5vw3DOOReJBwznnHOReMBwzjkXiQcM\n51xiPTokeLmk5wHDOZfUTj755Lg9tXNycrjzzjurdMwVK1bw9NNPV2nfI488slLbX3DBBXTp0oXM\nzEx69erFK6+8sss2d9xxB40aNeLJJ58skT5p0iQyMjLo2bMnRx55JB999FGV8hyVBwznXOLkPQv5\nc+DLt+GeHsFyNTEzdu7cyYwZM2jRokW1HRfKDxjlDWkOFHcErIxx48axcOFC/vrXv3L55ZeXWPfk\nk08yc+ZMPvnkE+666y5mz55dvK5Lly688cYbLFq0iJtvvplLL7200ueuDA8YzrnEyHsWXrwaCn8I\nljeuDJZ3I2isWLGCbt26cd5559GjRw9WrlxJ586dWbduHQB//OMfOfDAA+nfvz/Llv04Km68ocsh\nGHwwOzubQw89lIyMDB544AEArr/+et566y0yMzO55557eOyxxxg2bBjHH388AwYMYPPmzQwYMIA+\nffrQs2dPXnjhheJzFXX+e/311zn22GMZMWIEP//5zzn77LOpqKN06SHNZ8+ezRNPPMGMGTP42c9+\nxssvv8zvf//74juJI488kr333htI3JDmsbwfhnMuMV65DXaUGnxwx9YgPePMKh/2s88+4/HHH+fw\nww8vkT5v3jwmT57MwoULKSgooE+fPhxyyCFAMHT5Qw89xBFHHMH1119fvM/EiRNp3rw5c+bM4Ycf\nfqBfv34MHDiQsWPHcueddxaPWPvYY48xf/588vLy2GeffSgoKOD555/nJz/5CevWrePwww9n2LBh\nu8ybvWDBApYsWUL79u3p168f77zzDv379y+zbKWHND/hhBM44YQTipfbtGnDO++8E3ffRA1pHssD\nhnMuMTaW8W23rPSI9t9//12CBcBbb73FaaedVjyJUNFYSuUNXf7yyy+Tl5dXPDPexo0b+eyzz2jc\nuPEuxz/xxBPZZ599gKA67He/+x1vvvkmDRo0YNWqVaxZs4a2bduW2Kdv37507NgRgMzMTFasWBE3\nYGRnZ/O73/2O/Pz8XUbSjeK1115j4sSJvP3225XetzI8YDjnEqN5x6AaKl76bqiuwf0g+OC/7777\nGDRoUIn0119/vdzzTpo0ibVr1zJv3jwaNWpE586d2bZt2y77RB3SfNy4cYwYMYL77ruPiy66iHnz\n5kUuQ15eHhdffDH//Oc/admyZeT9qsLbMJxziTHgFmhUau6LRk2C9AQ4+uijmTZtGlu3bmXTpk28\n+OKLAOUOXT5o0CD+8Y9/sGPHDgA+/fRTtmzZEmlI8zZt2tCoUSNee+01vvzyy2opw+jRo9m5cyez\nZs2KtP1XX33F6aefzpNPPsmBBx5YLXkoj99hOOcSo6id4oXRQcN3805BsNiN9ovy9OnTh5EjR9Kr\nVy/atGnDoYceWryuaOjyBg0acMwxxxQPXX7xxRezYsUK+vTpg5nRunVrpk2bRkZGBmlpafTq1YsL\nLriguGG5yNlnn80pp5xCz549ycrK4uc//3m1lEESN910E3/5y192ueuJ57bbbmP9+vVcccUVADRs\n2JCqjNgdOX8+vHnyDL/sXG2ryvDmxZ32Lsyt/gxFVB+GLo/Chzd3ztVttRgoivjQ5dXDA4ZzLuX5\n0OXVI6GN3pIGS1om6XNJ18dZny1pYfhaLKlQ0j5R9nXOOVezEhYwJKUB9wMnAd2BX0rqHruNmY0z\ns0wzywRuAN4ws/9E2dfFN/KB94rbZJxzrjol8g6jL/C5mS03s+3AZODUcrb/JfBMFfd1zjmXYIkM\nGB2A2F47+WHaLiTtCQwG/q8K+14qaa6kuWvXrt3tTDvnqteFMy/kwpkX1nY2XDWoKx33TgHeMbP/\nVHZHM3vQzLLMLKt169YJyJpzri4pejw2kaZNm8bHH39c6f2mT5/O2LFjK7VPWloamZmZ9OjRg1NO\nOWWXodo3b95MVlYWXbt25euvvy6x7uyzz6Zbt2706NGDiy66qLgDYqIkMmCsAjrFLHcM0+IZxY/V\nUZXd1zlXR+UuzyVvbR5z18xl4NSB5C6v/UdsoygvYJQ3vPmwYcNKDG4YRZMmTVi4cCGLFy9mn332\n4f777y9xrjPPPJNzzz2XcePGceqpp/L9998Xrz/77LNZunQpixYtYuvWrTz88MOVOndlJTJgzAEO\nkNRFUmOCoDC99EaSmgPHAC9Udt/qMG3BKhZ8tYEP/v0f+o19lWkLPC7VBUv+1J8lfyp7VE9X9+Uu\nzyXn3Ry279wOwOotq8l5N6fagkZZQ4xv2bKFIUOG0KtXL3r06MGUKVOAYMjy7t27k5GRwW9/+1sg\nGC79+OOPJyMjgwEDBvDVV1/x7rvvMn36dLKzs8nMzOSLL77g2GOP5X/+53/Iyspi/PjxvPjiixx2\n2GH07t2bE044gTVr1gDBqLajR48GgomRrr76ao488ki6du1aPMBheUoPb37ZZZdx0kknMWbMGH7x\ni19w4403MmrUqOI7iZNPPhlJSKJv377JO7y5mRVIGg3MAtKAR8xsiaTLw/UTwk1PA142sy0V7Vvd\neZy2YBU3PLeI7YU7AVi1YSs3PLcIgOG94zaZOOciGj9/PNsKSw7It61wG+Pnj2dI192fsjU9PT3u\nEOMzZ86kffv25OYGgWnjxo2sX7+e559/nqVLlyKpuNrnqquu4vzzz+f888/nkUce4eqrr2batGkM\nGzaMoUOHMmLEiOLzbd++vXjYje+++473338fSTz88MP85S9/4a677tolj6tXr+btt99m6dKlDBs2\nrMTxSissLOSVV17hV7/6VXHaxIkTS2wzfPjwEsOfF9mxYwdPPvlkwnuvJ7TjnpnNAGaUSptQavkx\n4LEo+1a3cbOWsXVHYYm0rTsKGTdrmQcM53bTN1u+qVR6ZZU1xHjPnj255ppruO666xg6dChHHXUU\nBQUFpKen86tf/YqhQ4cydOhQAN577z2ee+45AM4991yuvfbaMs8X2/EvPz+fkSNHsnr1arZv306X\nLl3i7jN8+HAaNGhA9+7di+9CStu6dSuZmZmsWrWKgw46iBNPPLHSv4srrriCo48+mqOOOqrS+1ZG\nXWn0rhVfb9haqXTnXHRtm7atVHplxQ4xvnDhQvbdd1+2bdvGgQceyPz58+nZsyc33XQTt912Gw0b\nNuTDDz9kxIgRvPTSSwwePLjS54sd3vyqq65i9OjRLFq0iAceeCDu0OZQcnjzssbtK2rD+PLLLzGz\nEm0YUdx6662sXbuWu+++u1L7VUW9DhjtWzSpVLpzLroxfcaQnpZeIi09LZ0xfcZUy/HLGmL866+/\nZs899+Scc84hOzub+fPns3nzZjZu3MjJJ5/MPffcU2KK06LhzidNmlT8DT3K8OYdOgS1EI8//ni1\nlGfPPffk3nvv5a677qpw3vAiDz/8MLNmzeKZZ56hQYPEf5zX64CRPagbTRqllUhr0iiN7EHdailH\nzqWOIV2HkHNkDo0bBLPXtWvajpwjc6ql/QKCJ4Tmzp1Lz549eeKJJ4qHGF+0aBF9+/YlMzOTW2+9\nlZtuuolNmzYxdOhQMjIy6N+/f/G38fvuu49HH32UjIyMEm0Ao0aNYty4cfTu3Zsvvvhil3Pn5ORw\nxhlncMghh9CqVatqKQ9A7969ycjI4Jlnnql4Y+Dyyy9nzZo1HHHEEWRmZnLbbbdVW17iqffDm09b\nsIprp+axvXAnHVo0IXtQt6Ruv0iVodqLnpA6+HeJnXLSVU5Vhjcv6rT36OBHE5ElVwk+vPluGt67\nA898+BWQ/B+yztVFHihSR70PGK5uurXVZgCereV8OOd+VK/bMJxzlZdK1dj1SXVcNw8YzrnI0tPT\nWb9+vQeNJGNmrF+/nvT09Io3LodXSTmXYKnU6NuxY0fy8/PxkaGTT3p6Oh07dtytY3jAcM5F1qhR\nozJ7NbvU5wEjxdyyPjt854+jOueqlweMFONPF9U9H6/+vuKNnEsC3ujtnHMuEg8YzjnnIvEqKbze\n3znnovA7DOecc5F4wHDOOReJV0k5l2D779h1eGznkpHfYbg6Z9qCVSz86rd8uPx2+o19lWkLVtV2\nlpxz+B2Gq2OmLVjFDc8tYnvB3gCs2rCVG55bBJDU85Q4lwrKvcOQ9HNJAyQ1K5Ve+QlxnYtg3Kxl\nbN1RWCJt645Cxs1aVks5cs4VKTNgSLoaeAG4Clgs6dSY1X9KdMZc/fT1hq2VSnc1a+QD7xXP6ujq\nn/KqpC4BDjGzzZI6A1MldTaz8YBqInOu/mnfogmr4gSH9i2a1EJunHOxyquSamBmmwHMbAVwLHCS\npLvxgOESJHtQN5o0SiuR1qRRGtmDutVSjpxzRcoLGGskZRYthMFjKNAK6JnojLnKS4Wni4b37sAd\np/ekccPvAKNDiybccXpPb/B2rg4or0rqPKAgNsHMCoDzJD0Q5eBh4/h4IA142MzGxtnmWOCvQCNg\nnZkdE6avADYBhUCBmWVFOWd9lUpPFw3v3YGn59wJwLOXLqzl3LhYPoxO/VZmwDCz/HLWvVPRgSWl\nAfcDJwL5wBxJ083s45htWgB/Bwab2VeS2pQ6zHFmtq6ic7nyny5KtoDhnKubKuy4JymnisfuC3xu\nZsvNbDswGTi11DZnAc+Z2VcAZvZtFc9V7/nTRc65RCvvsdoGkiYCe1Tx2B2AlTHL+WFarAOBvSW9\nLmmepPNi1hkwO0y/tJx8XipprqS59Xme4bKeIvKni5xz1aW8O4yXgP+Y2Q0JPH9D4BBgCDAIuFnS\ngeG6/maWCZwEXCnp6HgHMLMHzSzLzLJat26dwKzWban0dFHu8lw+a1zIJ40LGTh1ILnLc2s7Sy50\na6vNxbM6uvqnvIBxCPDcbhx7FdApZrljmBYrH5hlZlvCtoo3gV4AZrYq/Pkt8DxBFZcrQ6o8XZS7\nPJect2+mQIBg9ZbV5Lx9swcNV628A2LVlBcwjgMelHRYFY89BzhAUhdJjYFRwPRS27wA9JfUUNKe\nwGHAJ5KaStoLQFJTYCCwuIr5qDeG9+5A5n530rfrTbxz/fFJFywAxr9/B9tsR4m0bbaD8e/fUUs5\ncs4VKe8pqY8lDSJorI5bHVQeMyuQNBqYRfBY7SNmtkTS5eH6CWb2iaSZQB6wk+DR28WSugLPSyrK\n49NmNrOyeXDJ55vtG0C79gv9ZvuGWsiNcy5WuaPVmtnXkoZU9eBmNgOYUSptQqnlccC4UmnLCaum\nXP3StqCQ1Y12/bNsW1AYZ2vnXE2qcHhzM9tUOk3SfkWPwjpXncb8kEZO2k62NfixtjR9507G/JBW\nzl7OVY53QKyaioY3P0LSiKIOdZIyJD0NVNhxL5kc3K45B7drXtvZcMCQo24h57vNtNtRgMxot6OA\nnO82M+SoW2o7a87VSTXZgF/mHYakcQRjRy0ErpM0C7gYuAO4qEZy5+qfjDMZAvR94XJaFhbSoHkn\nOGEcZJxZ2zlzrt4rr0pqCNDbzLZJ2pugE16PcORa5xIn40yufD+YcsXHkqo7iga33F7Qgn5jXyV7\nULekfBLPVV15VVLbzGwbgJl9B3zmwcK5+qnk4JYqHtwyGUdEdlVX3h1GV0mx/Sa6xC6b2bDEZatm\nXag1ADxay/lwrq7ywS0dlB8wSg8UeFciM+Kcq7tSbXDLouFNnq3lfCSb8jruvVGTGXHO1V0+da6D\nCMObO+eqLhVmQYTUGtwy1Zh+jenXNXKuCjvuOeeqJtVmQeyw8iXaz7uDdnzHt2rNyj7ZHNp7cG1n\nzdUgv8NwLkHKayhOOnnPcuii39NB39FA0Ja1HLro95DnrQD1SYV3GOH8FNnA/rHbm9nxCcyXq+ea\nNk7+m98cUbLhAAAVcklEQVSUaih+5TbYUSrfO7YG6d6pst6I8l/5v8AE4CHAR4BzLqKUaijemF+5\ndJeSolRJFZjZP8zsQzObV/RKeM6cS3Ip1VDcvGPl0uuwVHkQoTZECRgvSrpCUjtJ+xS9Ep4z55Jc\nqsyCCMCAW8j9SQsGdmxPRudODOzYntyftIAByTUopPdY3z1RqqTOD39mx6QZ0LX6s+NcahneuwNP\nz7kTSO5xsXKbNSWnVcvi2RBXN2pITquW0KwpVZ4wpxZ4j/XdU+Edhpl1ifPyYOFcPTJ+/vj4U+fO\nH19LOaqalHoQoRZEehRFUg+gO5BelGZmTyQqU66K8p7l/jVf0bKwEO7pEVQX+BMsrhp8s+WbSqXX\nVSn1IEItqPAOQ9LvgfvC13HAX4CUGXiQvGcZt/LfPLLis+BDNlmfK897Fl68mtaFhcFF3bgSXrw6\necvj6pS2TdtWKr2uyh7UjRGN3+XtxlezfI+zeLvx1Yxo/G5yPohQC6I0eo8ABgDfmNmFBHNtp8b0\ndOGHbKvCAgTJ/SFb3nPyrvaEd32Tv/53Un8hGdNnDOlp6SXS0tPSGdNnTC3lqGqGp73D2EYP07HB\nOhoIOjZYx9hGDzM8LaUmEU2YKAFjq5ntBAok/QT4FuiU2GzVkFT6kPXn5OueFLrrG9J1CDlH5tDQ\nAIN2TduRc2QOQ7omU5M38MptNCzcViKpYeG25PyfrwVRAsZcSS0IOu7NA+YDNTOBbKKl0odsCj0n\nnzJS6QsJQdA4YHsaB21P4+URLydfsIDU+p+n5vuURHlK6goz22BmE4ATgfPDqqnkl0ofsgNugUal\nGu4aNUm65+RTSop9OKWE5h3Jbbpnyf4kTfdMyv/52uhTEqXRW5LOkXRLOEXrBkl9E5ajmpRKH7IZ\nZ8Ip97I2LY2dAM07wSn3+lNStSmVvpCkiNzep5HTqiWrGzXEpOL+JLm9T6vtrFVabQxuGaVK6u/A\nEcAvw+VNwP0Jy1FNCj9k16U1xCD5P2QzzuTKffdjVPsu8JvFyVuOVJFKX0hSxPh1H7CtgUqkbWsg\nxq/7oJZyVHW10ackSsA4zMyuBLYBmNl3QOMoB5c0WNIySZ9Lur6MbY6VtFDSEklvVGbfapFxJtmd\nunBR5wP8Q9ZVr4wzye13CQM6hdUf+3Uit98l/jdWi1KlPwmU3XckkX1KogSMHZLSCIYDQVJrCGo9\nyhPucz9wEkGnv19K6l5qmxYEdzDDzOxg4Iyo+7rU9qjty6O2b21nY7fkLs8lJ38m3zYMqz/SRE7+\nTHKX59Z21qqsaeOGST30fKr0J4HaGdwySsC4F3geaCPpj8DbwJ8i7NcX+NzMlpvZdmAycGqpbc4C\nnjOzrwDM7NtK7OviSPZ/6FQyfv54tpV6hHNb4bakG04jlaRKfxIIxil74tAveWePK1m+x1m8nz6G\nJw79MqFjYlX4yWJmkyTNI+i8J2C4mX0S4dgdgJUxy/nAYaW2ORBoJOl1YC9gfDjkSJR9navTUqn6\nI1UUPQp805vXUwC0a9aOMX3GJOcjwnnP8u3nf+LGTnvyTcNmtC0oZMznf4K8vRNW7Rl1itY1wFvA\nu0ATSX2q6fwNgUOAIcAg4OZwhr/IJF0qaa6kuWvXrq2mbDm3+1Kp+iOVpER/EiD3rdvI2btZySe+\n9m5G7luJ6+cT5bHaPwB5BFVTd4WvOyMcexUle4R3DNNi5QOzzGyLma0D3iQYeiTKvgCY2YNmlmVm\nWa1bt46QLedqRipVf7i6Z/wehWxrUPIjfFuDBozfI3ETo0ap7D4T+GnYllAZc4ADJHUh+LAfRdBm\nEesF4G+SGhI8eXUYcA+wNMK+ztVpKVX94eqcbxqmVSq9OkQJGIuBFgRjSEVmZgWSRgOzgDTgETNb\nIunycP0EM/tE0kyCO5idwMNmthgg3r6VOb9zdcGQrkN4fPaNADw74uVazo1LJW0bt2D1jo1x0xMl\nSsC4A1ggaTHwQ1GimVU4xLmZzQBmlEqbUGp5HDAuyr7OuVrWtmdt58CFxhx+Azlv31xiYqt0NWLM\n4Tck7JxRAsbjwJ+BRUTof+Gccy7xaqPKM0rA+K+Z3ZuwHDgXz4XJ27nN1X2p0leppqs8o/zW3pJ0\nBzCdklVS8xOWK+ecc3VOlIDRO/x5eEyaAcdXf3acc87VVVF6eh9XExlxuy93eS55/MB2YODUgf4I\np3OuWqVGRZ4LBrp7N4ft4cjNq7esJufdHAAPGq7aPDr40drOgqtFUYcGcXWcD3TnnEs0Dxgpwge6\nc84lWpSxpPaUdLOkh8LlAyQNTXzWXGX4QHfOVULbnt4JsQqi3GE8SvA47RHh8irg9oTlyFWJD3Tn\nnEu0KI3ePzWzkZJ+CWBm/5WkinZyNauoYfuWN69nOz7QXV2SKp3EUok33ldNlDuM7ZKa8OMUrT8l\npgOfqzuGdB1CBnuQxR5JPc6/c65uivLV5/fATKCTpElAP+CCRGbKOedc3ROl496/JM0n6OktYEw4\n2ZFzzrlaVpNVnmWeKc40rKvDn/tJ2s/HknLOufqlvNB0VznrfCwp55yrZ8oMGD6GlHPOuVgVVn5J\nSgeuAPoT3Fm8BUwws23l7uiccy6lRGkteQLYBNwXLp8FPAmckahMOeecq3uiBIweZtY9Zvk1SR8n\nKkPOOecqoQaHOIkSMOZLOtzM3geQdBgwN7HZcs45F0VN9lqPEjAOAd6V9FW4vB+wTNIiwMwsI2G5\nc845V2dECRiDE54L55xzdV6Unt5fStob6BS7vXfcc865+iXKY7V/IBg76gvCAQjxjnvOOVfvRKmS\nOpNgiPPtic6Mc865uivK8OaLgRZVObikwZKWSfpc0vVx1h8raaOkheHrlph1KyQtCtP9qSyXvHx2\nN5ciotxh3AEskLSYmHkwzGxYeTtJSgPuB04E8oE5kqabWek+HG+ZWVlTvh5XIyPj+j+zc85VKErA\neBz4M7AI2FmJY/cFPjez5QCSJgOnAt7pzznnklCUgPFfM7u3CsfuAKyMWc4HDouz3ZGS8gjmCv+t\nmS0J0w2YLakQeMDMHox3EkmXApcC7LffflXIpnPOuSiitGG8JekOSUdI6lP0qqbzzwf2Czv/3QdM\ni1nX38wygZOAKyUdHe8AZvagmWWZWVbr1q0rnYHc5bnkrc1j7pq5DJw6kNzluVUohnPOpb4odxi9\nw5+Hx6RFeax2FUHfjSIdw7QfD2L2fcz7GZL+LqmVma0zs1Vh+reSnieo4nozQn4jy12eS867OWzf\nGTwAtnrLanLezQFI3vmwvT3GOZcgUTruVXVejDnAAZK6EASKUQQj3RaT1BZYY2YmqS/BHc96SU2B\nBma2KXw/ELitivko0/j549lWWHKU9m2F2xg/f3zyBgznnEuQSJPBShoCHAykF6WZWbkf4GZWIGk0\nMAtIAx4xsyWSLg/XTwBGAL+WVABsBUaFwWNf4HlJRXl82sxmVrp0FfhmyzeVSnfOufosSk/vCcCe\nwHHAwwQf8h9GObiZzQBmlEqbEPP+b8Df4uy3HOgV5Ry7o23TtqzesjpuunPOuZKiNHofaWbnAd+Z\n2a3AEcCBic1WzRjTZwzpaekl0tLT0hnTZ0wt5cg55+quKFVSW8Of/5XUHlgPtEtclmpOUTvFLe/c\nwvad22nXtB1j+ozx9gvnnIsjSsB4SVILYBzBY7BGUDWVEoZ0HcLUT6cCNTsRiXPOJZsoT0n9IXz7\nf5JeAtLNbGNis+Wcc66uqbANQ9IZkvYKF7OBRyX1Lm8f55xzqSdKo/fNYX+I/sAJwERgQgX7OOec\nSzFRAkZh+HMI8KCZ5QKNE5cl55xzdVGUgLFK0gPASGCGpD0i7ueccy6FRPngP5Ogt/YgM9sA7EPQ\nluGcc64eifKU1H+B52KWVwO7do92zjmX0rxqyTnnXCQeMJxzzkXiAcM551wkHjCcc85FEmk+DOdc\n1fkYZS5V+B2Gc865SDxgOOeci8QDhnPOuUg8YDjnnIvEA4ZzzrlIPGA455yLxAOGc865SDxgOOec\ni8QDhnPOuUg8YDjnnIskoQFD0mBJyyR9Lun6OOuPlbRR0sLwdUvUfZ1zztWshI0lJSkNuB84EcgH\n5kiabmYfl9r0LTMbWsV9nXPO1ZBE3mH0BT43s+Vmth2YDJxaA/s655xLgEQGjA7Aypjl/DCttCMl\n5Un6p6SDK7kvki6VNFfS3LVr11ZHvp1zzsVR243e84H9zCwDuA+YVtkDmNmDZpZlZlmtW7eu9gw6\n55wLJDJgrAI6xSx3DNOKmdn3ZrY5fD8DaCSpVZR9nXPO1axEBow5wAGSukhqDIwCpsduIKmtJIXv\n+4b5WR9lX+ecczUrYU9JmVmBpNHALCANeMTMlki6PFw/ARgB/FpSAbAVGGVmBsTdN1F5dc45V7GE\nTtEaVjPNKJU2Ieb934C/Rd3XOedc7antRm/nnHNJwgOGc865SDxgOOeciyShbRiu5j06+NHazoJz\nLkX5HYZzzrlIPGA455yLxAOGc865SDxgOOeci8QDhnPOuUg8YDjnnIvEA4ZzzrlIPGA455yLxAOG\nc865SDxgOOeci8QDhnPOuUg8YDjnnIvEA4ZzzrlIPGA455yLxAOGc865SDxgOOeci8QDhnPOuUg8\nYDjnnIvEA4ZzzrlIPGA455yLxAOGc865SDxgOOeci6RhIg8uaTAwHkgDHjazsWVsdyjwHjDKzKaG\naSuATUAhUGBmWYnK56ODH03UoZ1zLmUkLGBISgPuB04E8oE5kqab2cdxtvsz8HKcwxxnZusSlUfn\nnHPRJbJKqi/wuZktN7PtwGTg1DjbXQX8H/BtAvPinHNuNyUyYHQAVsYs54dpxSR1AE4D/hFnfwNm\nS5on6dKyTiLpUklzJc1du3ZtNWTbOedcPLXd6P1X4Doz2xlnXX8zywROAq6UdHS8A5jZg2aWZWZZ\nrVu3TmRenXOuXktko/cqoFPMcscwLVYWMFkSQCvgZEkFZjbNzFYBmNm3kp4nqOJ6M4H5dc45V45E\n3mHMAQ6Q1EVSY2AUMD12AzPrYmadzawzMBW4wsymSWoqaS8ASU2BgcDiBObVOedcBRJ2h2FmBZJG\nA7MIHqt9xMyWSLo8XD+hnN33BZ4P7zwaAk+b2cxE5dU551zFZGa1nYdqk5WVZXPnzq3tbDjnXNKQ\nNC9qP7fabvR2zjmXJFLqDkPSWuDLKu7eCkiVToKpUpZUKQd4WeqiVCkH7F5Z9jezSI+YplTA2B2S\n5iZy+JGalCplSZVygJelLkqVckDNlcWrpJxzzkXiAcM551wkHjB+9GBtZ6AapUpZUqUc4GWpi1Kl\nHFBDZfE2DOecc5H4HYZzzrlIPGA455yLpF4EDEmPSPpW0uKYtH0k/UvSZ+HPvWPW3SDpc0nLJA2q\nnVzHV0ZZciStkrQwfJ0cs65OlkVSJ0mvSfpY0hJJY8L0pLsu5ZQlGa9LuqQPJX0UluXWMD0Zr0tZ\nZUm66wLBZHOSFkh6KVyu+WtiZin/Ao4G+gCLY9L+Alwfvr8e+HP4vjvwEbAH0AX4Akir7TJUUJYc\n4Ldxtq2zZQHaAX3C93sBn4b5TbrrUk5ZkvG6CGgWvm8EfAAcnqTXpayyJN11CfP3/4CngZfC5Rq/\nJvXiDsPM3gT+Uyr5VODx8P3jwPCY9Mlm9oOZ/Rv4nGBo9TqhjLKUpc6WxcxWm9n88P0m4BOCCbaS\n7rqUU5ay1OWymJltDhcbhS8jOa9LWWUpS50ti6SOwBDg4ZjkGr8m9SJglGFfM1sdvv+GYIRciDBT\nYB11laS8sMqq6NY0KcoiqTPQm+AbYFJfl1JlgSS8LmHVx0KCaZP/ZWZJe13KKAsk33X5K3AtEDvZ\nXI1fk/ocMIpZcB+XzM8X/wPoCmQCq4G7ajc70UlqRjCn+/+Y2fex65LtusQpS1JeFzMrtGC2y45A\nX0k9Sq1PmutSRlmS6rpIGgp8a2bzytqmpq5JfQ4YayS1Awh/fhumR5kpsE4xszXhP8ZO4CF+vP2s\n02WR1IjgA3aSmT0XJifldYlXlmS9LkXMbAPwGjCYJL0uRWLLkoTXpR8wTNIKYDJwvKSnqIVrUp8D\nxnTg/PD9+cALMemjJO0hqQtwAPBhLeQvsqI/mtBp/Dg7YZ0tiyQBE4FPzOzumFVJd13KKkuSXpfW\nklqE75sAJwJLSc7rErcsyXZdzOwGM+towcyko4BXzewcauOa1HbLf028gGcIbj13ENTn/QpoCbwC\nfAbMBvaJ2f5GgicLlgEn1Xb+I5TlSWARkBf+sbSr62UB+hPcQucBC8PXycl4XcopSzJelwxgQZjn\nxcAtYXoyXpeyypJ01yUmf8fy41NSNX5NfGgQ55xzkdTnKinnnHOV4AHDOedcJB4wnHPOReIBwznn\nXCQeMJxzzkXiAcO5BJP0uqSs3TzGMEnXV1eenKuKhrWdAedcxcxsOkGfAedqjd9huHpHUlNJueE8\nCYsljQzTb5E0J0x7MOzBXXSHcI+kuZI+kXSopOfCeQhuD7fpLGmppEnhNlMl7Rnn3AMlvSdpvqT/\nDcefKr3N1Qrm1siTNDlMu0DS38L3C2NeWyUdE5bpEQXzPyyQdGoif4eufvKA4eqjwcDXZtbLzHoA\nM8P0v5nZoWFaE2BozD7bzSwLmEAwBMOVQA/gAkktw226AX83s4OA74ErYk8qqRVwE3CCmfUB5hLM\ncVDa9UBvM8sALi+90swyLRhQ7+bwGO8S9Ox91cz6AscB4yQ1rdRvxbkKeMBw9dEi4ERJf5Z0lJlt\nDNOPk/SBpEXA8cDBMftMj9l3iQVzYPwALOfHgd5Wmtk74funCIYMiXU4weQ274RDbp8P7B8nf3nA\nJEnnAAXxCiDpAGAccKaZ7QAGAteHx30dSAf2q+gX4VxleBuGq3fM7FNJfQjGe7pd0isEs5f9Hcgy\ns5WScgg+dIv8EP7cGfO+aLno/6j0ODull0UwJ8MvK8jiEIKZFU8BbpTUs8RBgmqsZ4FL7Mf5EAT8\nwsyWVXBs56rM7zBcvSOpPfBfM3uK4Ft6H34MDuvCD+QRVTj0fpKOCN+fBbxdav37QD9JPwvz0VTS\ngaXy1gDoZGavAdcBzYHS7RyPAI+a2VsxabMIJgUqanfpXYX8O1cuv8Nw9VFPgjr+nQSj/v7azDZI\neohgVNNvgDlVOO4y4EpJjwAfE0zUU8zM1kq6AHhG0h5h8k0Ec4AXSQOektSc4K7h3jBvAEjanyCY\nHSjponCfi4E/EMzKlhcGnX9Tsg3Gud3mo9U6Vw0UTM36Uthg7lxK8iop55xzkfgdhnPOuUj8DsM5\n51wkHjCcc85F4gHDOedcJB4wnHPOReIBwznnXCT/H+agTvEPTnylAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e96a3ee0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meanr2train = np.mean(r2train,axis = 2)\n",
    "meanr2test = np.mean(r2test,axis = 2)\n",
    "stdr2train = np.std(r2train,axis = 2)\n",
    "stdr2test = np.std(r2test,axis=2)\n",
    "plt.figure()\n",
    "lbl = ['linreg','ridge','lasso']\n",
    "for i in range(3):\n",
    "    plt.errorbar(samples,meanr2train[i,:],yerr = stdr2train[i,:],fmt = 'o',label = lbl[i]+'train R^2')\n",
    "    #plt.scatter(samples,meanr2test[i,:],label = lbl[i]+'test R^2')\n",
    "    plt.legend(loc = 'best')\n",
    "plt.title('Training Mean R^2 vs Sample Size')\n",
    "plt.xlabel('sample size')\n",
    "plt.ylabel('sample mean R^2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1e96c1b7470>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFNfiLEaVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e96a404a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "lbl = ['linreg','ridge','lasso']\n",
    "for i in range(3):\n",
    "    plt.errorbar(samples,meanr2test[i,:],yerr = stdr2test[i,:],fmt = 'o',label = lbl[i]+'test R^2')\n",
    "    plt.legend(loc = 'best')\n",
    "plt.title('Test Mean R^2 vs Sample Size')\n",
    "plt.xlabel('sample size')\n",
    "plt.ylabel('sample mean R^2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q\n",
    "\n",
    "- How do the training and test $R^2$ scores compare for the three methods? Give an explanation for your observations. How do the confidence intervals for the estimated $R^2$ change with training sample size? Based on the plots, which of the three methods would you recommend when one needs to fit a regression model using a small training sample?\n",
    "\n",
    "A\n",
    "\n",
    "- The training R^2 score for linear regression is better than lasso/ridge because they attempt to prevent overfitting. This means that they perform better in their test r^2 as we can see in the figures above. As sample size grows, the standard deviation of each model reduces since more samples in training should allow for a better understanding of the overall process.When one needs to fit a regression model using a small training sample they should opt for ridge since it performs relatively well on the training set but does well to prevent overfitting with a small sample size on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (g): Polynomial & Interaction Terms\n",
    "\n",
    "Moving beyond linear models, we will now try to improve the performance of the regression model in Part (b) from HW 3 by including higher-order polynomial and interaction terms. \n",
    "\n",
    "- For each continuous predictor $X_j$, include additional polynomial terms $X^2_j$, $X^3_j$, and $X^4_j$, and fit a multiple regression model to the expanded training set. How does the $R^2$ of this model on the test set compare with that of the linear model fitted in Part (b) from HW 3? Using a t-test, find out which of estimated coefficients for the polynomial terms are statistically significant at a significance level of 5%. \n",
    "\n",
    "- Fit a multiple linear regression model with additional interaction terms $\\mathbb{I}_{month = 12} \\times temp$ and $\\mathbb{I}_{workingday = 1} \\times \\mathbb{I}_{weathersit = 1}$ and report the test $R^2$ for the fitted model. How does this compare with the $R^2$ obtained using linear model in Part (b) from HW 3? Are the estimated coefficients for the interaction terms statistically significant at a significance level of 5%?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "# Pick out continuous variables, add polynomial terms\n",
    "X = X_train[['temp','atemp','humidity','windspeed']].values\n",
    "new_X = np.hstack((X**(i+2) for i in range(3)))\n",
    "new_X = np.append(X_train.values,new_X,axis = 1)\n",
    "Xt = X_test[['temp','atemp','humidity','windspeed']].values\n",
    "new_Xt = np.hstack((Xt**(i+2) for i in range(3)))\n",
    "new_Xt = np.append(X_test.values,new_Xt,axis = 1)\n",
    "A1 = np.copy(new_X)\n",
    "B1 = np.copy(new_Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2 of pol features = 0.669656240221\n",
      "Test R^2 of pol features = 0.29395929122\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.670</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.625</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   15.13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 11 Oct 2017</td> <th>  Prob (F-statistic):</th> <td>7.98e-50</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:25:11</td>     <th>  Log-Likelihood:    </th> <td> -2790.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   331</td>      <th>  AIC:               </th> <td>   5662.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   291</td>      <th>  BIC:               </th> <td>   5814.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    39</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 5035.2713</td> <td>  460.417</td> <td>   10.936</td> <td> 0.000</td> <td> 4129.101  5941.442</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>  766.4307</td> <td>  454.546</td> <td>    1.686</td> <td> 0.093</td> <td> -128.185  1661.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td> 1578.7544</td> <td>  519.364</td> <td>    3.040</td> <td> 0.003</td> <td>  556.569  2600.940</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td> 1523.2288</td> <td>  467.580</td> <td>    3.258</td> <td> 0.001</td> <td>  602.961  2443.496</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td> -325.0686</td> <td>  409.611</td> <td>   -0.794</td> <td> 0.428</td> <td>-1131.245   481.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td> -304.8491</td> <td>  446.028</td> <td>   -0.683</td> <td> 0.495</td> <td>-1182.700   573.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td> -418.0245</td> <td>  639.524</td> <td>   -0.654</td> <td> 0.514</td> <td>-1676.703   840.654</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>-1037.2042</td> <td>  677.186</td> <td>   -1.532</td> <td> 0.127</td> <td>-2370.008   295.599</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>-1456.1857</td> <td>  697.520</td> <td>   -2.088</td> <td> 0.038</td> <td>-2829.010   -83.362</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>-1416.9882</td> <td>  749.751</td> <td>   -1.890</td> <td> 0.060</td> <td>-2892.610    58.634</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>-1715.9389</td> <td>  743.240</td> <td>   -2.309</td> <td> 0.022</td> <td>-3178.747  -253.131</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>-1073.4008</td> <td>  660.859</td> <td>   -1.624</td> <td> 0.105</td> <td>-2374.069   227.268</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td> -925.8710</td> <td>  617.522</td> <td>   -1.499</td> <td> 0.135</td> <td>-2141.247   289.505</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td> -825.5328</td> <td>  591.138</td> <td>   -1.397</td> <td> 0.164</td> <td>-1988.981   337.916</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td> -555.6676</td> <td>  479.543</td> <td>   -1.159</td> <td> 0.248</td> <td>-1499.481   388.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td> -189.7675</td> <td>  365.157</td> <td>   -0.520</td> <td> 0.604</td> <td> -908.451   528.916</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td>  -93.3265</td> <td>  156.015</td> <td>   -0.598</td> <td> 0.550</td> <td> -400.387   213.734</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td> -133.4279</td> <td>  184.734</td> <td>   -0.722</td> <td> 0.471</td> <td> -497.012   230.156</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>  147.7313</td> <td>  195.071</td> <td>    0.757</td> <td> 0.449</td> <td> -236.197   531.660</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>   30.5924</td> <td>  187.547</td> <td>    0.163</td> <td> 0.871</td> <td> -338.528   399.713</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>  209.9371</td> <td>  182.024</td> <td>    1.153</td> <td> 0.250</td> <td> -148.313   568.187</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td>  471.0834</td> <td>  246.557</td> <td>    1.911</td> <td> 0.057</td> <td>  -14.178   956.345</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td>  351.2739</td> <td>  150.615</td> <td>    2.332</td> <td> 0.020</td> <td>   54.841   647.707</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td>   59.0119</td> <td>  196.208</td> <td>    0.301</td> <td> 0.764</td> <td> -327.155   445.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>-1043.9997</td> <td>  546.051</td> <td>   -1.912</td> <td> 0.057</td> <td>-2118.709    30.710</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td>  771.4866</td> <td>  760.117</td> <td>    1.015</td> <td> 0.311</td> <td> -724.536  2267.510</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>  897.2756</td> <td>  713.172</td> <td>    1.258</td> <td> 0.209</td> <td> -506.353  2300.904</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td> -668.9145</td> <td>  157.356</td> <td>   -4.251</td> <td> 0.000</td> <td> -978.615  -359.214</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td> -446.5085</td> <td>  148.929</td> <td>   -2.998</td> <td> 0.003</td> <td> -739.623  -153.394</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td>-1811.0180</td> <td>  816.910</td> <td>   -2.217</td> <td> 0.027</td> <td>-3418.820  -203.216</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td> 1175.5005</td> <td>  788.864</td> <td>    1.490</td> <td> 0.137</td> <td> -377.102  2728.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>   <td>  -53.6709</td> <td>  155.383</td> <td>   -0.345</td> <td> 0.730</td> <td> -359.488   252.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>   <td>  -34.1653</td> <td>  126.952</td> <td>   -0.269</td> <td> 0.788</td> <td> -284.026   215.695</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>   <td>    8.6078</td> <td>  275.731</td> <td>    0.031</td> <td> 0.975</td> <td> -534.071   551.287</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>   <td> -303.9358</td> <td>  246.097</td> <td>   -1.235</td> <td> 0.218</td> <td> -788.292   180.420</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>   <td>  -16.0576</td> <td>   44.892</td> <td>   -0.358</td> <td> 0.721</td> <td> -104.412    72.297</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>   <td>   44.8339</td> <td>   65.459</td> <td>    0.685</td> <td> 0.494</td> <td>  -83.999   173.667</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>   <td>  -45.1910</td> <td>  171.419</td> <td>   -0.264</td> <td> 0.792</td> <td> -382.570   292.188</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>   <td>  -20.7686</td> <td>  147.605</td> <td>   -0.141</td> <td> 0.888</td> <td> -311.276   269.739</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>   <td>  -24.8367</td> <td>   31.481</td> <td>   -0.789</td> <td> 0.431</td> <td>  -86.796    37.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>   <td>  -20.1769</td> <td>   30.327</td> <td>   -0.665</td> <td> 0.506</td> <td>  -79.864    39.510</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>29.995</td> <th>  Durbin-Watson:     </th> <td>   1.959</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  10.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.094</td> <th>  Prob(JB):          </th> <td> 0.00609</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.161</td> <th>  Cond. No.          </th> <td>1.35e+16</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.670\n",
       "Model:                            OLS   Adj. R-squared:                  0.625\n",
       "Method:                 Least Squares   F-statistic:                     15.13\n",
       "Date:                Wed, 11 Oct 2017   Prob (F-statistic):           7.98e-50\n",
       "Time:                        17:25:11   Log-Likelihood:                -2790.9\n",
       "No. Observations:                 331   AIC:                             5662.\n",
       "Df Residuals:                     291   BIC:                             5814.\n",
       "Df Model:                          39                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "const       5035.2713    460.417     10.936      0.000      4129.101  5941.442\n",
       "x1           766.4307    454.546      1.686      0.093      -128.185  1661.046\n",
       "x2          1578.7544    519.364      3.040      0.003       556.569  2600.940\n",
       "x3          1523.2288    467.580      3.258      0.001       602.961  2443.496\n",
       "x4          -325.0686    409.611     -0.794      0.428     -1131.245   481.108\n",
       "x5          -304.8491    446.028     -0.683      0.495     -1182.700   573.002\n",
       "x6          -418.0245    639.524     -0.654      0.514     -1676.703   840.654\n",
       "x7         -1037.2042    677.186     -1.532      0.127     -2370.008   295.599\n",
       "x8         -1456.1857    697.520     -2.088      0.038     -2829.010   -83.362\n",
       "x9         -1416.9882    749.751     -1.890      0.060     -2892.610    58.634\n",
       "x10        -1715.9389    743.240     -2.309      0.022     -3178.747  -253.131\n",
       "x11        -1073.4008    660.859     -1.624      0.105     -2374.069   227.268\n",
       "x12         -925.8710    617.522     -1.499      0.135     -2141.247   289.505\n",
       "x13         -825.5328    591.138     -1.397      0.164     -1988.981   337.916\n",
       "x14         -555.6676    479.543     -1.159      0.248     -1499.481   388.146\n",
       "x15         -189.7675    365.157     -0.520      0.604      -908.451   528.916\n",
       "x16          -93.3265    156.015     -0.598      0.550      -400.387   213.734\n",
       "x17         -133.4279    184.734     -0.722      0.471      -497.012   230.156\n",
       "x18          147.7313    195.071      0.757      0.449      -236.197   531.660\n",
       "x19           30.5924    187.547      0.163      0.871      -338.528   399.713\n",
       "x20          209.9371    182.024      1.153      0.250      -148.313   568.187\n",
       "x21          471.0834    246.557      1.911      0.057       -14.178   956.345\n",
       "x22          351.2739    150.615      2.332      0.020        54.841   647.707\n",
       "x23           59.0119    196.208      0.301      0.764      -327.155   445.179\n",
       "x24        -1043.9997    546.051     -1.912      0.057     -2118.709    30.710\n",
       "x25          771.4866    760.117      1.015      0.311      -724.536  2267.510\n",
       "x26          897.2756    713.172      1.258      0.209      -506.353  2300.904\n",
       "x27         -668.9145    157.356     -4.251      0.000      -978.615  -359.214\n",
       "x28         -446.5085    148.929     -2.998      0.003      -739.623  -153.394\n",
       "x29        -1811.0180    816.910     -2.217      0.027     -3418.820  -203.216\n",
       "x30         1175.5005    788.864      1.490      0.137      -377.102  2728.103\n",
       "x31          -53.6709    155.383     -0.345      0.730      -359.488   252.146\n",
       "x32          -34.1653    126.952     -0.269      0.788      -284.026   215.695\n",
       "x33            8.6078    275.731      0.031      0.975      -534.071   551.287\n",
       "x34         -303.9358    246.097     -1.235      0.218      -788.292   180.420\n",
       "x35          -16.0576     44.892     -0.358      0.721      -104.412    72.297\n",
       "x36           44.8339     65.459      0.685      0.494       -83.999   173.667\n",
       "x37          -45.1910    171.419     -0.264      0.792      -382.570   292.188\n",
       "x38          -20.7686    147.605     -0.141      0.888      -311.276   269.739\n",
       "x39          -24.8367     31.481     -0.789      0.431       -86.796    37.122\n",
       "x40          -20.1769     30.327     -0.665      0.506       -79.864    39.510\n",
       "==============================================================================\n",
       "Omnibus:                       29.995   Durbin-Watson:                   1.959\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               10.202\n",
       "Skew:                          -0.094   Prob(JB):                      0.00609\n",
       "Kurtosis:                       2.161   Cond. No.                     1.35e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 2.1e-28. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xf1 = sm.add_constant(new_X)\n",
    "os1 = sm.OLS(y_train,Xf1).fit()\n",
    "print('Train R^2 of pol features = %s' %r2_score(y_train,os1.predict(sm.add_constant(new_X))))\n",
    "print('Test R^2 of pol features = %s' %r2_score(y_test,os1.predict(sm.add_constant(new_Xt))))\n",
    "os1.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q\n",
    "\n",
    "- How does the $R^2$ of this model on the test set compare with that of the linear model fitted in Part (b) from HW 3? Using a t-test, find out which of estimated coefficients for the polynomial terms are statistically significant at a significance level of 5%. \n",
    "\n",
    "A\n",
    "\n",
    "- The R^2 of this model on the test set yields 0.293 (see top) while in 3b it gave us 0.257. This shows us that polynomial features improve the test score. Using a t-test shows us that only temp^2 is statistically significant from the new set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interaction + Polynomial Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2 of p + interaction = 0.669967599304\n",
      "Test R^2 of p + interaction = 0.298749520353\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.670</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.623</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   14.31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 11 Oct 2017</td> <th>  Prob (F-statistic):</th> <td>1.07e-48</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:25:18</td>     <th>  Log-Likelihood:    </th> <td> -2790.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   331</td>      <th>  AIC:               </th> <td>   5665.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   289</td>      <th>  BIC:               </th> <td>   5825.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    41</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 5002.8722</td> <td>  480.670</td> <td>   10.408</td> <td> 0.000</td> <td> 4056.815  5948.929</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>  763.6379</td> <td>  463.746</td> <td>    1.647</td> <td> 0.101</td> <td> -149.110  1676.386</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td> 1564.2796</td> <td>  545.007</td> <td>    2.870</td> <td> 0.004</td> <td>  491.593  2636.966</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td> 1493.1626</td> <td>  505.353</td> <td>    2.955</td> <td> 0.003</td> <td>  498.524  2487.801</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td> -325.7395</td> <td>  415.794</td> <td>   -0.783</td> <td> 0.434</td> <td>-1144.107   492.628</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td> -311.7359</td> <td>  465.982</td> <td>   -0.669</td> <td> 0.504</td> <td>-1228.885   605.413</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td> -429.5823</td> <td>  676.680</td> <td>   -0.635</td> <td> 0.526</td> <td>-1761.428   902.264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>-1024.8594</td> <td>  711.364</td> <td>   -1.441</td> <td> 0.151</td> <td>-2424.970   375.251</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>-1466.6541</td> <td>  734.348</td> <td>   -1.997</td> <td> 0.047</td> <td>-2912.002   -21.306</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>-1427.6739</td> <td>  798.423</td> <td>   -1.788</td> <td> 0.075</td> <td>-2999.136   143.788</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>-1720.6375</td> <td>  791.728</td> <td>   -2.173</td> <td> 0.031</td> <td>-3278.922  -162.353</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>-1055.9093</td> <td>  726.079</td> <td>   -1.454</td> <td> 0.147</td> <td>-2484.983   373.165</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td> -882.2336</td> <td>  692.220</td> <td>   -1.274</td> <td> 0.204</td> <td>-2244.665   480.198</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td> -773.6133</td> <td>  656.762</td> <td>   -1.178</td> <td> 0.240</td> <td>-2066.257   519.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td> -548.4481</td> <td>  515.138</td> <td>   -1.065</td> <td> 0.288</td> <td>-1562.346   465.450</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td> -174.5488</td> <td>  369.198</td> <td>   -0.473</td> <td> 0.637</td> <td> -901.206   552.109</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td> -107.4572</td> <td>  158.815</td> <td>   -0.677</td> <td> 0.499</td> <td> -420.038   205.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td> -148.1951</td> <td>  187.684</td> <td>   -0.790</td> <td> 0.430</td> <td> -517.595   221.205</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>  129.0956</td> <td>  199.548</td> <td>    0.647</td> <td> 0.518</td> <td> -263.656   521.847</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>   12.1349</td> <td>  191.518</td> <td>    0.063</td> <td> 0.950</td> <td> -364.813   389.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>  191.8714</td> <td>  185.980</td> <td>    1.032</td> <td> 0.303</td> <td> -174.176   557.918</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td>  462.4486</td> <td>  248.640</td> <td>    1.860</td> <td> 0.064</td> <td>  -26.926   951.824</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td>  251.9984</td> <td>  243.915</td> <td>    1.033</td> <td> 0.302</td> <td> -228.076   732.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td>  181.8735</td> <td>  309.316</td> <td>    0.588</td> <td> 0.557</td> <td> -426.924   790.671</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td> -923.0406</td> <td>  594.656</td> <td>   -1.552</td> <td> 0.122</td> <td>-2093.447   247.366</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td>  795.0787</td> <td>  764.545</td> <td>    1.040</td> <td> 0.299</td> <td> -709.704  2299.861</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>  881.7153</td> <td>  717.029</td> <td>    1.230</td> <td> 0.220</td> <td> -529.546  2292.976</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td> -676.3519</td> <td>  158.504</td> <td>   -4.267</td> <td> 0.000</td> <td> -988.320  -364.384</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td> -447.0797</td> <td>  150.175</td> <td>   -2.977</td> <td> 0.003</td> <td> -742.655  -151.505</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td>-1801.8581</td> <td>  819.867</td> <td>   -2.198</td> <td> 0.029</td> <td>-3415.526  -188.190</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td> 1170.6729</td> <td>  792.462</td> <td>    1.477</td> <td> 0.141</td> <td> -389.055  2730.401</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>   <td>  -56.5186</td> <td>  155.980</td> <td>   -0.362</td> <td> 0.717</td> <td> -363.519   250.482</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>   <td>  -37.8573</td> <td>  127.862</td> <td>   -0.296</td> <td> 0.767</td> <td> -289.515   213.801</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>   <td>    1.6628</td> <td>  278.742</td> <td>    0.006</td> <td> 0.995</td> <td> -546.958   550.284</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>   <td> -298.9891</td> <td>  247.662</td> <td>   -1.207</td> <td> 0.228</td> <td> -786.439   188.461</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>   <td>  -15.0311</td> <td>   45.150</td> <td>   -0.333</td> <td> 0.739</td> <td> -103.896    73.834</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>   <td>   44.5758</td> <td>   65.712</td> <td>    0.678</td> <td> 0.498</td> <td>  -84.760   173.911</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>   <td>  -44.7557</td> <td>  171.933</td> <td>   -0.260</td> <td> 0.795</td> <td> -383.155   293.643</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>   <td>  -21.0470</td> <td>  148.500</td> <td>   -0.142</td> <td> 0.887</td> <td> -313.326   271.231</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>   <td>  -23.9877</td> <td>   31.681</td> <td>   -0.757</td> <td> 0.450</td> <td>  -86.343    38.367</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>   <td>  -19.3589</td> <td>   30.476</td> <td>   -0.635</td> <td> 0.526</td> <td>  -79.341    40.623</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th>   <td>    3.9679</td> <td>   63.938</td> <td>    0.062</td> <td> 0.951</td> <td> -121.875   129.811</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th>   <td>  165.0265</td> <td>  322.219</td> <td>    0.512</td> <td> 0.609</td> <td> -469.167   799.220</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>29.023</td> <th>  Durbin-Watson:     </th> <td>   1.960</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  10.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.099</td> <th>  Prob(JB):          </th> <td> 0.00647</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.168</td> <th>  Cond. No.          </th> <td>1.36e+16</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.670\n",
       "Model:                            OLS   Adj. R-squared:                  0.623\n",
       "Method:                 Least Squares   F-statistic:                     14.31\n",
       "Date:                Wed, 11 Oct 2017   Prob (F-statistic):           1.07e-48\n",
       "Time:                        17:25:18   Log-Likelihood:                -2790.7\n",
       "No. Observations:                 331   AIC:                             5665.\n",
       "Df Residuals:                     289   BIC:                             5825.\n",
       "Df Model:                          41                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "const       5002.8722    480.670     10.408      0.000      4056.815  5948.929\n",
       "x1           763.6379    463.746      1.647      0.101      -149.110  1676.386\n",
       "x2          1564.2796    545.007      2.870      0.004       491.593  2636.966\n",
       "x3          1493.1626    505.353      2.955      0.003       498.524  2487.801\n",
       "x4          -325.7395    415.794     -0.783      0.434     -1144.107   492.628\n",
       "x5          -311.7359    465.982     -0.669      0.504     -1228.885   605.413\n",
       "x6          -429.5823    676.680     -0.635      0.526     -1761.428   902.264\n",
       "x7         -1024.8594    711.364     -1.441      0.151     -2424.970   375.251\n",
       "x8         -1466.6541    734.348     -1.997      0.047     -2912.002   -21.306\n",
       "x9         -1427.6739    798.423     -1.788      0.075     -2999.136   143.788\n",
       "x10        -1720.6375    791.728     -2.173      0.031     -3278.922  -162.353\n",
       "x11        -1055.9093    726.079     -1.454      0.147     -2484.983   373.165\n",
       "x12         -882.2336    692.220     -1.274      0.204     -2244.665   480.198\n",
       "x13         -773.6133    656.762     -1.178      0.240     -2066.257   519.030\n",
       "x14         -548.4481    515.138     -1.065      0.288     -1562.346   465.450\n",
       "x15         -174.5488    369.198     -0.473      0.637      -901.206   552.109\n",
       "x16         -107.4572    158.815     -0.677      0.499      -420.038   205.124\n",
       "x17         -148.1951    187.684     -0.790      0.430      -517.595   221.205\n",
       "x18          129.0956    199.548      0.647      0.518      -263.656   521.847\n",
       "x19           12.1349    191.518      0.063      0.950      -364.813   389.082\n",
       "x20          191.8714    185.980      1.032      0.303      -174.176   557.918\n",
       "x21          462.4486    248.640      1.860      0.064       -26.926   951.824\n",
       "x22          251.9984    243.915      1.033      0.302      -228.076   732.073\n",
       "x23          181.8735    309.316      0.588      0.557      -426.924   790.671\n",
       "x24         -923.0406    594.656     -1.552      0.122     -2093.447   247.366\n",
       "x25          795.0787    764.545      1.040      0.299      -709.704  2299.861\n",
       "x26          881.7153    717.029      1.230      0.220      -529.546  2292.976\n",
       "x27         -676.3519    158.504     -4.267      0.000      -988.320  -364.384\n",
       "x28         -447.0797    150.175     -2.977      0.003      -742.655  -151.505\n",
       "x29        -1801.8581    819.867     -2.198      0.029     -3415.526  -188.190\n",
       "x30         1170.6729    792.462      1.477      0.141      -389.055  2730.401\n",
       "x31          -56.5186    155.980     -0.362      0.717      -363.519   250.482\n",
       "x32          -37.8573    127.862     -0.296      0.767      -289.515   213.801\n",
       "x33            1.6628    278.742      0.006      0.995      -546.958   550.284\n",
       "x34         -298.9891    247.662     -1.207      0.228      -786.439   188.461\n",
       "x35          -15.0311     45.150     -0.333      0.739      -103.896    73.834\n",
       "x36           44.5758     65.712      0.678      0.498       -84.760   173.911\n",
       "x37          -44.7557    171.933     -0.260      0.795      -383.155   293.643\n",
       "x38          -21.0470    148.500     -0.142      0.887      -313.326   271.231\n",
       "x39          -23.9877     31.681     -0.757      0.450       -86.343    38.367\n",
       "x40          -19.3589     30.476     -0.635      0.526       -79.341    40.623\n",
       "x41            3.9679     63.938      0.062      0.951      -121.875   129.811\n",
       "x42          165.0265    322.219      0.512      0.609      -469.167   799.220\n",
       "==============================================================================\n",
       "Omnibus:                       29.023   Durbin-Watson:                   1.960\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               10.080\n",
       "Skew:                          -0.099   Prob(JB):                      0.00647\n",
       "Kurtosis:                       2.168   Cond. No.                     1.36e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 2.08e-28. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interaction terms + polynomial terms\n",
    "temp1 = np.array([pd.get_dummies(Biketrain['month'])[12]])\n",
    "temp2 = np.array([Biketrain['temp']])\n",
    "new_X = np.hstack((new_X,temp1.T*temp2.T))\n",
    "temp1 = np.array([pd.get_dummies(Biketrain['workingday'])[1]])\n",
    "temp2 = np.array([pd.get_dummies(Biketrain['weather'])[1]])\n",
    "new_X = np.hstack((new_X,temp1.T*temp2.T))\n",
    "\n",
    "temp1 = np.array([pd.get_dummies(Biketest['month'])[12]])\n",
    "temp2 = np.array([Biketest['temp']])\n",
    "new_Xt = np.hstack((new_Xt,temp1.T*temp2.T))\n",
    "temp1 = np.array([pd.get_dummies(Biketest['workingday'])[1]])\n",
    "temp2 = np.array([pd.get_dummies(Biketest['weather'])[1]])\n",
    "new_Xt = np.hstack((new_Xt,temp1.T*temp2.T))\n",
    "\n",
    "X = sm.add_constant(new_X)\n",
    "ols = sm.OLS(y_train,X).fit()\n",
    "print('Train R^2 of p + interaction = %s' %r2_score(y_train,ols.predict(sm.add_constant(new_X))))\n",
    "print('Test R^2 of p + interaction = %s' %r2_score(y_test,ols.predict(sm.add_constant(new_Xt))))\n",
    "ols.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q\n",
    "\n",
    "- How does this compare with the $R^2$ obtained using linear model in Part (b) from HW 3? Are the estimated coefficients for the interaction terms statistically significant at a significance level of 5%?\n",
    "\n",
    "A\n",
    "\n",
    "- The test R^2 of 0.298 is higher with these two interaction terms than the R^2 of 0.257 of part 3b. Neither of the coefficients are statistically significant at the 5% level. Adding interaction terms makes the difference of R^2 from 0.293 to 0.298"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (h): PCA to deal with high dimensionality\n",
    "\n",
    "We would like to fit a model to include all main effects, polynomial terms up to the $4^{th}$ order, and all interactions between all possible predictors and polynomial terms (not including the interactions between $X^1_j$, $X^2_j$, $X^3_j$, and $X^4_j$ as they would just create higher order polynomial terms).  \n",
    "\n",
    "- Create an expanded training set including all the desired terms mentioned above.  What are the dimensions of this 'design matrix' of all the predictor variables?   What are the issues with attempting to fit a regression model using all of these predictors?\n",
    "\n",
    "- Instead of using the usual approaches for model selection, let's instead use principal components analysis (PCA) to fit the model.  First, create the principal component vectors in python (consider: should you normalize first?).  Then fit 5 different regression models: (1) using just the first PCA vector, (2) using the first two PCA vectors, (3) using the first three PCA vectors, etc...  Briefly summarize how these models compare in the training set.\n",
    "\n",
    "- Use the test set to decide which of the 5 models above is best to predict out of sample.  How does this model compare to the previous models you've fit?  What are the interpretations of this model's coefficients?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 1: Use all polynomial features generated by the function\n",
    "\n",
    "Pavlos said many approaches can work, therefore I decided to compare two methods.\n",
    "\n",
    "The first is simply to determine every interaction term across the 28 initial features we had using polynomial features (a quick and dirty method). This means that some columns will have zeros and that terms such as x1^2x3 exist. PCA probably handles the columns where there are zeros well but it is unclear how it handles continuous polynomial features with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(331, 35960)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_exp = PolynomialFeatures(4).fit_transform(X_train)\n",
    "X_test_exp = PolynomialFeatures(4).fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2: 0.17755647284025688\n",
      "Test R^2: 0.1334272656417913\n",
      "Train R^2: 0.3101639313048855\n",
      "Test R^2: 0.18655009403485423\n",
      "Train R^2: 0.31016520046265195\n",
      "Test R^2: 0.18663082519320406\n",
      "Train R^2: 0.31709960037180795\n",
      "Test R^2: 0.18194035482808146\n",
      "Train R^2: 0.36586631796099633\n",
      "Test R^2: 0.21601386039653636\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "X_normalizedtr = preprocessing.normalize(X_train_exp, norm='l2')\n",
    "X_normalizedtst = preprocessing.normalize(X_test_exp, norm='l2')\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(X_normalizedtr)\n",
    "X_train_pca = pca.transform(X_normalizedtr)\n",
    "X_test_pca = pca.transform(X_normalizedtst)\n",
    "\n",
    "for i in range(5):\n",
    "    regression_model = LinearRegression(fit_intercept=True)\n",
    "    regression_model.fit(X_train_pca[:,0:i+1], y_train)\n",
    "    print('Train R^2: {}'.format(regression_model.score(X_train_pca[:,0:i+1],y_train)))\n",
    "    print('Test R^2: {}'.format(regression_model.score(X_test_pca[:,0:i+1], y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 2: Find up to 4th order interactions + 12 polynomial continuous features - zero columns\n",
    "\n",
    "In this option we restrict polynomial features to only give us interaction terms(e.g. no z^2 terms which were problematic in option 1). For example, in option 1 we had season2 and season2^2, these are essentially the same predictors but we have removed those in this step.\n",
    "\n",
    "Secondly, we add in our 12 polynomial features from the continuous variables e.g. X^2,X^3,X^4 where X consists of temp,atemp,humidity and windspeed.\n",
    "\n",
    "Lastly, we drop any columns with zero values e.g. season2*season3 gives zero since these two are linearly independent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xpol_train = []\n",
    "Xpol_test = []\n",
    "\n",
    "# 1. include interaction between all 28 features e.g x1x2,x1x2x3x4 etc this is all interactions of order 2,3 and 4\n",
    "Xpol_train.append(PolynomialFeatures(4,interaction_only=True).fit_transform(X_train))\n",
    "Xpol_test.append(PolynomialFeatures(4,interaction_only=True).fit_transform(X_test))\n",
    "\n",
    "# 2. add polynomial features e.g. temp^2 for cts variables\n",
    "X = X_train[['temp','atemp','humidity','windspeed']].values\n",
    "Xpol_train.append(np.hstack((X**(i+2) for i in range(3))))\n",
    "X = X_test[['temp','atemp','humidity','windspeed']].values\n",
    "Xpol_test.append(np.hstack((X**(i+2) for i in range(3))))\n",
    "\n",
    "Xpol_trn = np.concatenate((Xpol_train[0],Xpol_train[1]),axis=1)\n",
    "Xpol_tst = np.concatenate((Xpol_test[0],Xpol_test[1]),axis=1)\n",
    "dropidx = np.any(Xpol_trn,0)\n",
    "Xpol_trn = Xpol_trn[:,dropidx]\n",
    "Xpol_tst = Xpol_tst[:,dropidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(331, 3889)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(Xpol_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2: 0.01713893713748793\n",
      "Test R^2: 0.01659706324271959\n",
      "Train R^2: 0.3196281405663829\n",
      "Test R^2: 0.1976541751664117\n",
      "Train R^2: 0.3226069768381997\n",
      "Test R^2: 0.20945444493588328\n",
      "Train R^2: 0.32622477573156816\n",
      "Test R^2: 0.21376775964700634\n",
      "Train R^2: 0.5207583019702792\n",
      "Test R^2: 0.2504529807432241\n"
     ]
    }
   ],
   "source": [
    "X_normalizedtr = preprocessing.normalize(Xpol_trn, norm='l2')\n",
    "X_normalizedtst = preprocessing.normalize(Xpol_tst, norm='l2')\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(X_normalizedtr)\n",
    "X_train_pca = pca.transform(X_normalizedtr)\n",
    "X_test_pca = pca.transform(X_normalizedtst)\n",
    "\n",
    "for i in range(5):\n",
    "    regression_model = LinearRegression(fit_intercept=True)\n",
    "    regression_model.fit(X_train_pca[:,0:i+1], y_train)\n",
    "    print('Train R^2: {}'.format(regression_model.score(X_train_pca[:,0:i+1],y_train)))\n",
    "    print('Test R^2: {}'.format(regression_model.score(X_test_pca[:,0:i+1], y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: What we find is that using polynomial features directly gives us many more features than option 2 described above. In the training and test, both options appear to perform similarly but the real difference comes in with 5 PCA components in option 2 indicating that the additional cross terms in option 1 may be hindering the PCA.\n",
    "\n",
    "\n",
    "Q\n",
    "\n",
    "- Create an expanded training set including all the desired terms mentioned above.  What are the dimensions of this 'design matrix' of all the predictor variables?   What are the issues with attempting to fit a regression model using all of these predictors?\n",
    "\n",
    "A\n",
    "\n",
    "- Dimension of the design matrix varies based on which terms you decide to keep. In option 1 above the order should be ${k+n}\\choose{n} $ where k is the number of features and n is the order. This turns out to be 35960 in our case. Another way to think of this is to say if you have 4 features, and you want interaction terms of the order 2 then you will get $4\\choose 2$ terms, lets say you alsow wanted order 3 features from this set then you will get $4\\choose 3$ features. You can then add these up based on the given problem. Note- this choose way of thinking does not account for interactions between a predictor and itself e.g. x1^2 is not accounted for.\n",
    "\n",
    "\n",
    "Many issues arise while trying to fit with all predictors - \n",
    "1. over fitting on the training set\n",
    "2. multi collinearity issues\n",
    "3. features that are no longer intuitive e.g (weather*dayofweek*month*temp) seems like this could be a scaled proxy for temp\n",
    "4. computational complexity scales exponentially\n",
    "\n",
    "Q\n",
    "\n",
    "- Instead of using the usual approaches for model selection, let's instead use principal components analysis (PCA) to fit the model.  First, create the principal component vectors in python (consider: should you normalize first?).  Then fit 5 different regression models: (1) using just the first PCA vector, (2) using the first two PCA vectors, (3) using the first three PCA vectors, etc...  Briefly summarize how these models compare in the training set.\n",
    "\n",
    "A\n",
    "\n",
    "- Each PCA component adds an orthoganal feature to the variable space. The components are selected by finding the predictor combination that describes most of the variance within the predictor space. The training R^2 appear to improve with additional components in this case with a significant jump from 1 PCA component to 2.\n",
    "\n",
    "Q\n",
    "\n",
    "- Use the test set to decide which of the 5 models above is best to predict out of sample.  How does this model compare to the previous models you've fit?  What are the interpretations of this model's coefficients?\n",
    "\n",
    "A\n",
    "\n",
    "- The best test R^2 is with 5 pca components - However, in comparison to other models it doesn't perform significantly better. For example the test R^2 on forward selection from hw3 was 0.285 but here we get 0.25 with 5 features. \n",
    "\n",
    "- Since PCA is a projection, we tend to lose information about our initial prediction space labels so it is unclear what the coefficients mean.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (i): Beyond Squared Error\n",
    "\n",
    "We have seen in class that the multiple linear regression method optimizes the Mean Squared Error (MSE) on the training set. Consider the following alternate evaluation metric, referred to as the Root Mean Squared Logarthmic Error (RMSLE):\n",
    "\n",
    "$$\n",
    "\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (log(y_i+1) - log(\\hat{y}_i+1))^2}.\n",
    "$$\n",
    "\n",
    "The *lower* the RMSLE the *better* is the performance of a model. The RMSLE penalizes errors on smaller responses more heavily than errors on larger responses. For example, the RMSLE penalizes a prediction of $\\hat{y} = 15$ for a true response of $y=10$ more heavily than a prediction of $\\hat{y} = 105$ for a true response of $100$, though the difference in predicted and true responses are the same in both cases. \n",
    "\n",
    "This is a natural evaluation metric for bike share demand prediction, as in this application, it is more important that the prediction model is accurate on days where the demand is low (so that the few customers who arrive are served satisfactorily), compared to days on which the demand is high (when it is less damaging to lose out on some customers).\n",
    "\n",
    "The following code computes the RMSLE for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------  rmsle\n",
    "# A function for evaluating Root Mean Squared Logarithmic Error (RMSLE)\n",
    "# of the linear regression model on a data set\n",
    "# Input: \n",
    "#      y_test (n x 1 array of response variable vals in testing data)\n",
    "#      y_pred (n x 1 array of response variable vals in testing data)\n",
    "# Return: \n",
    "#      RMSLE (float) \n",
    "\n",
    "def rmsle(y, y_pred):     \n",
    "    # Evaluate sqaured error, against target labels\n",
    "    # rmsle = \\sqrt(1/n \\sum_i (log (y[i]+1) - log (y_pred[i]+1))^2)\n",
    "    rmsle_ = np.sqrt(np.mean(np.square(np.log(y+1) - np.log(y_pred+1))))\n",
    "    \n",
    "    return rmsle_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the above code to compute the training and test RMSLE for the polynomial regression model you fit in Part (g). \n",
    "\n",
    "You are required to develop a strategy to fit a regression model by optimizing the RMSLE on the training set. Give a justification for your proposed approach. Does the model fitted using your approach yield lower train RMSLE than the model in Part (g)? How about the test RMSLE of the new model? \n",
    "\n",
    "**Note:** We do not require you to implement a new regression solver for RMSLE. Instead, we ask you to think about ways to use existing built-in functions to fit a model that performs well on RMSLE. Your regression model may use the same polynomial terms used in Part (g)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Reg + polynomial features score on RMSLE\n",
    "\n",
    "Note that I had exactly the same features in hw3 as the solution set - using that I simply extended the space to include 12 additional features and ran the regression. What I found was that 3 of the values were negative (which would blow up the log function) so I casted them to zero because we cannot have a negative count. You will find that others who used precisely the same features as hw3 will have a similar issue - Rohan on Piazza suggested that it is fine to cast these to zero!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training rmsle = 0.721457659701\n",
      "Test rmsle = 0.844336732793\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "new_X = A1\n",
    "new_Xt = B1\n",
    "ypred = os1.predict(sm.add_constant(new_X))\n",
    "ypred[ypred<0] = 0\n",
    "print('Training rmsle = %s'%rmsle(y_train,ypred))\n",
    "\n",
    "ypred = os1.predict(sm.add_constant(new_Xt))\n",
    "ypred[(np.where(ypred<-1))] = 0\n",
    "print('Test rmsle = %s'%rmsle(y_test,ypred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To fit a regression to optimize RMSLE, I simply went back to the definition of the function and said XB = Y, therefore I want to optimize for B(coefficients) such that the rmsle function is a minimum. Using scipy.optimize I could feed in my rmsle function and an initial guess of coefficients and the result converged to a set of coefficients and a minimum rmsle of 0.309 - much better than the training rmsle from the earlier part of this problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "# add intercept term to train/test\n",
    "z1 = np.array([np.ones(331)])\n",
    "new_X = np.append(z1.T,new_X,axis =1 )\n",
    "z2 = np.array([np.ones(400)])\n",
    "new_Xt = np.append(z2.T,new_Xt,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 0.3092854042796907\n",
       " hess_inv: array([[  5.59980012e+06,  -1.33359064e+06,   2.14510107e+06, ...,\n",
       "          2.47391966e+04,   1.44972911e+05,  -3.91535960e+05],\n",
       "       [ -1.33359064e+06,   1.23116994e+06,  -8.62606859e+05, ...,\n",
       "          8.77735811e+04,  -2.07924531e+05,   1.00706331e+05],\n",
       "       [  2.14510107e+06,  -8.62606859e+05,   9.67044953e+05, ...,\n",
       "          1.55575022e+03,   1.25903177e+05,  -1.42620245e+05],\n",
       "       ..., \n",
       "       [  2.47391966e+04,   8.77735811e+04,   1.55575022e+03, ...,\n",
       "          4.32242098e+05,  -7.09931765e+03,   7.32312286e+04],\n",
       "       [  1.44972911e+05,  -2.07924531e+05,   1.25903177e+05, ...,\n",
       "         -7.09931765e+03,   8.45111058e+04,  -1.35125938e+04],\n",
       "       [ -3.91535960e+05,   1.00706331e+05,  -1.42620245e+05, ...,\n",
       "          7.32312286e+04,  -1.35125938e+04,   1.11538357e+05]])\n",
       "      jac: array([ -6.31064177e-06,  -1.09151006e-06,  -1.63912773e-07,\n",
       "        -1.10641122e-06,   1.49011612e-07,   5.81145287e-07,\n",
       "        -4.23565507e-06,   1.01700425e-06,   1.22934580e-06,\n",
       "        -2.49966979e-06,   2.32458115e-06,   1.97440386e-06,\n",
       "         1.39698386e-06,   1.04308128e-06,  -6.29574060e-07,\n",
       "         2.19047070e-06,   7.82310963e-08,   2.92435288e-06,\n",
       "        -3.67686152e-06,   4.39584255e-07,   4.02331352e-07,\n",
       "         3.27453017e-06,  -2.02655792e-06,   9.16421413e-06,\n",
       "        -2.52202153e-06,   3.23727727e-06,   2.79396772e-06,\n",
       "        -2.80141830e-06,   2.09361315e-06,   7.11530447e-07,\n",
       "        -3.25962901e-06,  -6.56768680e-06,   9.97632742e-06,\n",
       "        -3.22610140e-06,  -2.83122063e-06,  -5.92321157e-07,\n",
       "        -2.14576721e-06,  -4.02331352e-06,  -1.58324838e-06,\n",
       "        -4.84287739e-08,  -1.99303031e-06])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 16039\n",
       "      nit: 339\n",
       "     njev: 373\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([ 2935.11730484,   519.33166188,   687.57773576,  1070.16883965,\n",
       "         159.67987187,   563.20203739,   218.76627866,   392.83235698,\n",
       "          72.61514752,   122.13879957,    92.0639083 ,   603.92117117,\n",
       "         547.47068025,   245.72986653,  -165.21903291,    25.72754025,\n",
       "         225.46334236,    52.78674642,   142.16449505,    18.56511633,\n",
       "         249.03084334,   622.8108474 ,   659.22672798,   255.94932226,\n",
       "        -685.31067612,   724.24430823,   832.1777933 ,  -632.29961888,\n",
       "        -491.56699298,   -60.67553633,    52.41870372,   273.33506721,\n",
       "         144.78182604,   -35.47135386,  -282.31883408,   -10.01739528,\n",
       "          72.13350783,  -308.39972369,   146.51080614,   -58.48793872,\n",
       "         -51.85335338])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define rmsle as a function of B and cast any -ve predictions to 0\n",
    "def rmsle(B):     \n",
    "    # Evaluate sqaured error, against target labels\n",
    "    # rmsle = \\sqrt(1/n \\sum_i (log (y[i]+1) - log (y_pred[i]+1))^2)\n",
    "    y_pred = np.dot(new_X,B)\n",
    "    y_pred[y_pred<=-1] = 0\n",
    "    y = y_train\n",
    "    rmsle_ = np.sqrt(np.mean(np.square(np.log(y+1) - np.log(y_pred+1))))\n",
    "    \n",
    "    return rmsle_\n",
    "\n",
    "optimize.minimize(rmsle,np.ones(41),method = 'BFGS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the coefficients from this function, we can then simply check what our test rmsle is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE on Training = 0.30928540428\n",
      "RMSLE on Test = 0.712888559774\n"
     ]
    }
   ],
   "source": [
    "B = np.array([ 2935.11730484,   519.33166188,   687.57773576,  1070.16883965,\n",
    "         159.67987187,   563.20203739,   218.76627866,   392.83235698,\n",
    "          72.61514752,   122.13879957,    92.0639083 ,   603.92117117,\n",
    "         547.47068025,   245.72986653,  -165.21903291,    25.72754025,\n",
    "         225.46334236,    52.78674642,   142.16449505,    18.56511633,\n",
    "         249.03084334,   622.8108474 ,   659.22672798,   255.94932226,\n",
    "        -685.31067612,   724.24430823,   832.1777933 ,  -632.29961888,\n",
    "        -491.56699298,   -60.67553633,    52.41870372,   273.33506721,\n",
    "         144.78182604,   -35.47135386,  -282.31883408,   -10.01739528,\n",
    "          72.13350783,  -308.39972369,   146.51080614,   -58.48793872,\n",
    "         -51.85335338])\n",
    "\n",
    "def rmsle(y, y_pred):     \n",
    "    # Evaluate sqaured error, against target labels\n",
    "    # rmsle = \\sqrt(1/n \\sum_i (log (y[i]+1) - log (y_pred[i]+1))^2)\n",
    "    rmsle_ = np.sqrt(np.mean(np.square(np.log(y+1) - np.log(y_pred+1))))\n",
    "    \n",
    "    return rmsle_\n",
    "ypred = np.dot(new_X,B)\n",
    "print('RMSLE on Training = %s'%rmsle(y_train,ypred))\n",
    "ypred = np.dot(new_Xt,B)\n",
    "ypred[ypred<-1] = 0\n",
    "print('RMSLE on Test = %s'%rmsle(y_test,ypred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this method - both test and training rmsle reduce which indicate the fact that the coefficients are better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (j): Dealing with Erroneous Labels\n",
    "\n",
    "Due to occasional system crashes, some of the bike counts reported in the data set have been recorded manually. These counts are not very unreliable and are prone to errors. It is known that roughly 5% of the labels in the training set are erroneous (i.e. can be arbitrarily different from the true counts), while all the labels in the test set were confirmed to be accurate. Unfortunately, the identities of the erroneous records in the training set are not available. Can this information about presence of 5% errors in the training set labels (without details about the specific identities of the erroneous rows) be used to improve the performance of the model in Part (g)? Note that we are interested in improving the $R^2$ performance of the model on the test set (not the training $R^2$ score). \n",
    "\n",
    "As a final task, we require you to come up with a strategy to fit a regression model, taking into account the errors in the training set labels. Explain the intuition behind your approach (we do not expect a detailed mathematical justification). Use your approach to fit a regression model on the training set, and compare its test $R^2$ with the model in Part (g).\n",
    "\n",
    "**Note:** Again, we do not require you to implement a new regression solver for handling erroneous labels. It is sufficient that you to come up with an approach that uses existing built-in functions. Your regression model may use the same polynomial terms used in Part (g)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "One potential method is to fit the model using the entire training set and then look at residuals since the 5% of false data should have an extra error term which will allow us to distinguish those data points. However, taking such an approach means we actually use the data with errors to run a model which will bias our coefficients and may lead us to pick out observations that were actually recorded accurately.\n",
    "\n",
    "An alternative and better methodology that avoids having to look at residuals (which isn't a good method since the training set is influenced by the 5%) is to iteratively train the model by leaving 1 observation out and using backward selection on the observations to determine the best test R^2 with 5% of 331 =16 of the worst features removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_new = A1\n",
    "X_newtst = B1\n",
    "xworstidx = []\n",
    "linreg = LinearRegression(fit_intercept =True)\n",
    "dummy = np.copy(X_new)\n",
    "ydummy = np.copy(y_train)\n",
    "for i in range(16):\n",
    "    score = []\n",
    "    for j in range(np.shape(dummy)[0]):\n",
    "        xtemp = np.delete(dummy,j,axis = 0)\n",
    "        ytemp = np.delete(ydummy,j,axis=0)\n",
    "        rez = linreg.fit(xtemp,ytemp)\n",
    "        score.append(rez.score(xtemp,ytemp))\n",
    "    dummy = np.delete(dummy,np.argmin(score),axis = 0)\n",
    "    ydummy = np.delete(ydummy,np.argmin(score),axis = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train R^2 score = 0.601211747402\n",
      "test R^2 score = 0.298701532812\n"
     ]
    }
   ],
   "source": [
    "rez = linreg.fit(dummy,ydummy)\n",
    "print('train R^2 score = %s'%rez.score(dummy,ydummy))\n",
    "print('test R^2 score = %s'%rez.score(X_newtst,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test R^2 improves marginally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "--_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APCOMP209a - Homework Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##  Question 1: Student's t MLE\n",
    "\n",
    "Use Maximum Likelihood Estimation to generate a linear regression model on the data provided in ``beerdata.csv`` considering two statistical models for noise: a) iid Normal and b) iid Student's t-distribution with $\\nu=5$ and scale factor =0.5.  \n",
    "\n",
    "Compare the two models performances and comment why it is perhaps appropriate to use the Student's t-distribution instead of the Normal? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "HINTS:\n",
    "1. Use the probability density function for the Student's t distribution  with location  and scale factor .\n",
    "2. If the MLE regressions coefficients can not be derived analytically consider numerical methods.\n",
    "3. You can use sklearn or statsmodel for the Normal case \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (continued from HW2) \n",
    "\n",
    "Read sections 1 and 2 of this [paper](https://www.researchgate.net/profile/Roberto_Togneri/publication/45094554_Linear_Regression_for_Face_Recognition/links/09e4150d243bd8b987000000/Linear-Regression-for-Face-Recognition.pdf). \n",
    "\n",
    "Briefly, the model leverages the concept that \"patterns from a single-object class lie on a linear subspace.\"   It also makes use of the idea of linear regression as a problem about projections.  In this case, given a vector $y$, the goal is to find the subspace induced by $\\mathrm{Col} \\, \\mathbf X$ that produced the 'closest' projection vector $\\widehat y$ to the original $y$.  \n",
    "\n",
    "### Question 2a\n",
    "\n",
    "As discussed in the paper, our face dataset contains cleaned images of faces belonging to different people. Assuming that patterns (faces) from one class (person) are elements of the same subspace, let's try to classify an unknown face using the method presented in the paper.  For each class $i$, we need to:\n",
    "\n",
    "1. construct the $\\mathbf H_i$ hat matrix from known faces, being careful to follow the column concatenation step described in the paper to convert an image into its vector representation;\n",
    "2. calculate the predicted $\\widehat y_i$, the closest vector in $\\mathrm{Col} \\, \\mathbf X_i$ to $y$; and\n",
    "3. calculate the magnitude of the difference vector between $y$ and $\\widehat y_i$.\n",
    "\n",
    "You should then be able to make a classification decision.\n",
    "\n",
    "**Notes:**\n",
    "- Use the provided code to download and re-sample the dataset.\n",
    "- Follow the normalisation step in the paper to ensure the \"maximum pixel value is 1\".\n",
    "- Your classifier should have approximately an 80% accuracy.\n",
    "- Use the image plotting library of matplotlib to display one (or two) correctly classified faces and the known faces.\n",
    "- Use the image plotting library of matplotlib to display one (or two) incorrectly classified faces and the known faces.\n",
    "    \n",
    "### Question 2b - Significant Faces\n",
    "Select an example of a correctly classified face. Use statsmodels to investigate the most predictive columns (faces) that the model used in this regression:\n",
    "\n",
    "(i) Which columns (i.e. faces) make the highest contribution to the projection?\n",
    "\n",
    "(ii) Which columns (i.e. faces) are the least useful in making this projection?\n",
    "\n",
    "Plot the correctly assigned face, and the two faces from the questions (i) and (ii). What do you notice about these faces?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "# Note that you may need to run the following command to install Python Image Library (PIL)\n",
    "#pip install Pillow\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# starter functions provided to students\n",
    "def rgb2gray(rgb):\n",
    "    '''\n",
    "    function to convert RGB image to gray scale\n",
    "    accepts 3D numpy array and returns 2D array with same dimensions\n",
    "    as the first two dimensions of input\n",
    "    '''\n",
    "    \n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "def fetch_and_read_data(shape=(50,30)):\n",
    "    \n",
    "    '''\n",
    "    Function to download image data, store in a local folder (note this is 18.4mb), only download the data when\n",
    "    the local folder is not present, read in the images, downsample them to the specified shape (default = (50x30) (rows x cols))\n",
    "    and finally split them into a four tuple return object.\n",
    "    \n",
    "    Returns:\n",
    "        - 1) training image data (i.e. images that should form the predictor matrix in your solution)\n",
    "        - 2) training image data labels (i.e. labels from 1 to 50 that identify which face (1) belongs to)\n",
    "        - 3) testing image data (i.e. data that you should use to try and classify - note this forms the predictor variable in your regression)\n",
    "        - 4) testing image data labels (i.e. the labels for (3) - this is to allow you to evaluate your model)\n",
    "    \n",
    "    ___________________\n",
    "    Aside:\n",
    "    If you want to change the sampling dimensions of your data, pass the shape = (x,y) argument to the method where\n",
    "    y is the number of columns and x is the number of rows in the image.\n",
    "    '''\n",
    "    \n",
    "    if not os.path.exists('./cropped_faces'):\n",
    "        url = urllib.request.urlopen(\"http://www.anefian.com/research/GTdb_crop.zip\")\n",
    "        \n",
    "        zipfile = ZipFile(BytesIO(url.read()))\n",
    "        zipfile.extractall()\n",
    "     \n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    files = os.listdir('cropped_faces')\n",
    "    for f in files:\n",
    "        if '.jpg' in f:\n",
    "            image = Image.open('cropped_faces/' + f)\n",
    "            image = image.resize((shape[1], shape[0]))\n",
    "            data.append(rgb2gray(np.asarray(image)))\n",
    "            labels.append(int(f.split('_')[0][1:]) - 1)\n",
    "            \n",
    "    data = np.array(data)\n",
    "    \n",
    "    trainX, testX, trainY, testY = train_test_split(data, labels, test_size=0.2, stratify=labels)\n",
    "    return np.array(trainX), np.array(testX), np.array(trainY), np.array(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiQAAAEyCAYAAADKlMtrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d663630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# starter code for the students\n",
    "train_dataset, test_dataset, train_labels, test_labels = fetch_and_read_data()\n",
    "\n",
    "# code to plot some of the images\n",
    "fig, axes = plt.subplots(2,4,figsize=(10,5))\n",
    "axes = axes.flatten()\n",
    "[axes[i].imshow(train_dataset[i], cmap='gray') for i in range(len(axes))]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
